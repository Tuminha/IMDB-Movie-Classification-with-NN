{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî§ Notebook 02: Preprocessing & Tokenization\n",
        "\n",
        "## From Raw Text to Processable Tokens\n",
        "\n",
        "This notebook teaches you how to transform raw movie review text into a format that neural networks can understand. You'll build a tokenization function that splits text into individual words while handling common preprocessing challenges.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Concept Primer: Why Tokenization Matters\n",
        "\n",
        "### What We're Doing\n",
        "Converting raw text strings into lists of individual words (tokens) that can be processed by neural networks.\n",
        "\n",
        "### Why This Step is Critical\n",
        "**Neural networks can't process raw text directly.** They need:\n",
        "- **Fixed-size inputs** (tokens, not variable-length strings)\n",
        "- **Numerical representations** (integers, not text)\n",
        "- **Consistent format** (lowercase, no punctuation)\n",
        "\n",
        "### What We'll Build\n",
        "- **Tokenization function** using regex to extract words\n",
        "- **Text normalization** (lowercase conversion)\n",
        "- **Corpus creation** (list of tokenized reviews)\n",
        "\n",
        "### Common Pitfalls\n",
        "- **Forgetting to lowercase** creates duplicate vocabulary entries (\"Movie\" ‚â† \"movie\")\n",
        "- **Inconsistent tokenization** leads to vocabulary bloat\n",
        "- **Special character handling** can break downstream processing\n",
        "\n",
        "### How It Maps to Our Pipeline\n",
        "Raw text ‚Üí `tokenize_review()` ‚Üí List of tokens ‚Üí Vocabulary building (next notebook)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #1: Implement Tokenization Function\n",
        "\n",
        "**Task:** Create a function that splits text into individual words using regex.\n",
        "\n",
        "**Hint:** Use `re.findall(r'\\b\\w+\\b', text.lower())` to extract word tokens and convert to lowercase.\n",
        "\n",
        "**Expected Function Signature:**\n",
        "```python\n",
        "def tokenize_review(text):\n",
        "    # Your implementation here\n",
        "    return tokens  # List of strings\n",
        "```\n",
        "\n",
        "**Expected Output Example:**\n",
        "```python\n",
        "tokenize_review(\"This movie was amazing!\")\n",
        "# Returns: ['this', 'movie', 'was', 'amazing']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #1: Implement tokenization function\n",
        "import re\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #2: Create Tokenized Corpus\n",
        "\n",
        "**Task:** Apply your tokenization function to all training texts to create a corpus of tokenized reviews.\n",
        "\n",
        "**Hint:** Use list comprehension: `tokenized_corpus = [tokenize_review(text) for text in train_texts]`\n",
        "\n",
        "**Expected Variable:**\n",
        "- `tokenized_corpus` ‚Üí List of lists, where each inner list contains tokens from one review\n",
        "\n",
        "**Expected Output Example:**\n",
        "```python\n",
        "tokenized_corpus[:3]\n",
        "# Returns: [['this', 'movie', 'was', 'amazing'], ['terrible', 'plot', 'bad', 'acting'], ['love', 'this', 'film']]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #2: Create tokenized corpus\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Reflection Prompts\n",
        "\n",
        "### ü§î Understanding Check\n",
        "1. **Why remove punctuation?** What would happen if you kept punctuation marks as separate tokens?\n",
        "\n",
        "2. **What information do we lose by lowercasing?** Think about proper nouns and acronyms.\n",
        "\n",
        "3. **How does the regex pattern `r'\\b\\w+\\b'` work?** What does `\\b` mean in regex?\n",
        "\n",
        "4. **Why is consistent tokenization important?** What happens if the same word gets tokenized differently in different contexts?\n",
        "\n",
        "### üéØ Tokenization Quality\n",
        "- Do your tokenized examples look reasonable?\n",
        "- Are there any edge cases your tokenizer might not handle well?\n",
        "- How might this tokenization approach differ from what TinyBERT will do?\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflections here:**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
