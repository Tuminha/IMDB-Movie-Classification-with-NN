{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìù Lab Notes: Your Learning Journal\n",
        "\n",
        "## Reflection Solidifies Learning\n",
        "\n",
        "This notebook serves as your personal learning journal. Research shows that reflection is one of the most powerful ways to solidify understanding and identify patterns in your learning process.\n",
        "\n",
        "**How to use this notebook:**\n",
        "- Complete at least one entry per notebook you finish\n",
        "- Be honest about what worked and what didn't\n",
        "- Notice patterns across your learning journey\n",
        "- Use insights to improve your approach\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Entry Template\n",
        "\n",
        "**Date:** [Today's date]  \n",
        "**Notebook:** [Notebook number and name]  \n",
        "**What Worked:** [What went well, what you understood easily]  \n",
        "**What Broke:** [What was difficult, where you got stuck]  \n",
        "**One Surprise:** [Something unexpected you learned or discovered]  \n",
        "**One Change for Next Time:** [How you'll approach similar problems differently]\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Entry 1: Notebook 00 - Overview\n",
        "\n",
        "**Date:** [Fill in when you complete this notebook]  \n",
        "**Notebook:** 00_overview.ipynb  \n",
        "**What Worked:**  \n",
        "**What Broke:**  \n",
        "**One Surprise:**  \n",
        "**One Change for Next Time:**  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Entry 2: Notebook 01 - Import and Inspect\n",
        "\n",
        "**Date:** October 23, 2025  \n",
        "**Notebook:** 01_import_and_inspect.ipynb  \n",
        "\n",
        "**What Worked:**  \n",
        "Loading and inspecting the data was straightforward. Understanding that `aspect_encoded` represents the numerical labels (0, 1, 2) for the three categories (Cinematography, Characters, Story) was intuitive once explained.\n",
        "\n",
        "**What Broke:**  \n",
        "Initial confusion about what `aspect_encoded` meant - it looked like random numbers (0, 1, 2, 2, 2) until I realized it was the encoded categorical labels.\n",
        "\n",
        "**One Surprise:**  \n",
        "The dataset is quite small (369 training, ~130 test samples) - much smaller than I expected for a machine learning task. Also surprised that there are only 3 aspect categories, making this a manageable multi-class classification problem.\n",
        "\n",
        "**One Change for Next Time:**  \n",
        "Would spend more time upfront understanding the data dictionary and what each column represents before diving into exploration.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Entry 3: Notebook 02 - Preprocessing & Tokenization\n",
        "\n",
        "**Date:** October 23, 2025  \n",
        "**Notebook:** 02_preprocessing_tokenization.ipynb  \n",
        "\n",
        "**What Worked:**  \n",
        "The basic tokenization with `re.findall(r'\\\\b\\\\w+\\\\b', text.lower())` worked well for splitting text into words.\n",
        "\n",
        "**What Broke:**  \n",
        "Regex is really confusing! I don't understand what `\\\\b` does and there are so many special characters that I just ask an LLM to do it - I don't think memorizing regex patterns makes sense when an LLM can spit it out in seconds. Also noticed that \"let's\" splits into \"let\" and \"s\", where \"s\" alone provides no information. Emojis are deleted entirely.\n",
        "\n",
        "**One Surprise:**  \n",
        "How much information gets lost with simple tokenization - contractions split poorly, emojis disappear, and we lose context. TinyBERT will handle these cases better (maybe \"Lets\" instead of \"let\"+\"s\", and emojis converted to text).\n",
        "\n",
        "**One Change for Next Time:**  \n",
        "Would explore different tokenization strategies earlier to understand the trade-offs. Maybe compare word-level vs character-level vs subword tokenization.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Entry 4: Notebook 03 - Vocabulary Building & Encoding\n",
        "\n",
        "**Date:** October 23, 2025  \n",
        "**Notebook:** 03_vocab_and_encoding.ipynb  \n",
        "\n",
        "**What Worked:**  \n",
        "Building the vocabulary with `Counter` and limiting to top 1000 words made sense. Using special tokens `<PAD>` and `<UNK>` is a clever way to handle padding and unknown words.\n",
        "\n",
        "**What Broke:**  \n",
        "Lots of confusion about vocab creation! Why start at index 2? (Answer: to not overwrite `<PAD>=0` and `<UNK>=1`). How to handle test data with unseen words? (Answer: map to `<UNK>`). The vocab only has 1002 words total, which seems very limiting.\n",
        "\n",
        "**One Surprise:**  \n",
        "Most frequent words are stopwords (\"the\", \"a\", \"and\") which don't carry much meaning. I expected more movie-specific words. Also surprised that limiting to 1000 words means ~30-50% of test words map to `<UNK>` - that's a LOT of lost information!\n",
        "\n",
        "**One Change for Next Time:**  \n",
        "Would experiment with different vocab sizes (500, 2000, 5000) to see the trade-off between model size and performance. Also consider filtering stopwords to keep more domain-specific terms.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Entry 5: Notebook 04 - Padding, Tensors & DataLoader\n",
        "\n",
        "**Date:** October 23, 2025  \n",
        "**Notebook:** 04_padding_tensors_dataloader.ipynb  \n",
        "\n",
        "**What Worked:**  \n",
        "Padding sequences to a fixed length (128) makes sense for batch processing. Converting to PyTorch tensors with `torch.long` was straightforward.\n",
        "\n",
        "**What Broke:**  \n",
        "Still confused about why we need tensors and what `torch.long` means exactly - they're still just arrays to me. Also unclear why shuffle=True is important (Answer: prevents the NN from learning sequence patterns in the data order). The math of why batch_size=16 is chosen wasn't clear.\n",
        "\n",
        "**One Surprise:**  \n",
        "Padding to max_len=128 means shorter reviews get lots of `<PAD>` tokens - the model needs to learn to ignore these! Also surprising that we process samples in batches rather than one at a time - it's more efficient but adds complexity. The dataset should be divisible by batch size for clean processing.\n",
        "\n",
        "**One Change for Next Time:**  \n",
        "Would experiment with different batch sizes (8, 32, 64) and max_length values (64, 256) to understand their impact on training speed and memory usage.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Entry 6: Notebook 05 - Simple Neural Network\n",
        "\n",
        "**Date:** October 23, 2025  \n",
        "**Notebook:** 05_simpleNN_with_embedding.ipynb  \n",
        "\n",
        "**What Worked:**  \n",
        "Understanding that the embedding layer learns spatial representations of words where semantic meaning = mathematical distance. The forward pass flow (embedding ‚Üí pooling ‚Üí fc1 ‚Üí relu ‚Üí fc2) made sense eventually.\n",
        "\n",
        "**What Broke:**  \n",
        "Really struggled to define the `SimpleNN` class! The forward pass 3D ‚Üí 2D tensor transformation was confusing. Where does the mask go? How does pooling work? Why do we need it? Took a lot of guidance to understand mean pooling averages embeddings while masking out padding.\n",
        "\n",
        "**One Surprise:**  \n",
        "The embedding layer (embed_size=50) means each word gets 50 numbers to represent it - these are learned during training! Also surprising that we mask padding by multiplying by 0 rather than removing it. Mean pooling is smarter than I thought - it handles variable-length sequences elegantly.\n",
        "\n",
        "**One Change for Next Time:**  \n",
        "Would draw out the tensor shapes at each step (before coding) to visualize the transformations. Also would read more about different pooling strategies (max, mean, last token).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Entry 7: Notebook 06 - Training & Evaluation\n",
        "\n",
        "**Date:** October 23, 2025  \n",
        "**Notebook:** 06_eval_baseline_metrics.ipynb  \n",
        "\n",
        "**What Worked:**  \n",
        "The training loop structure made sense (forward pass ‚Üí loss ‚Üí backward ‚Üí optimizer step). Training was fast (~2 minutes for 50 epochs). Visualizing loss and accuracy curves helped understand overfitting.\n",
        "\n",
        "**What Broke:**  \n",
        "Hit two bugs: `RuntimeError` about FloatTensor vs LongTensor (duplicate embedding call), and `AttributeError` for missing `pad_id` attribute. Also struggled to understand confusion matrices - they're really hard to read! Which axis is which?\n",
        "\n",
        "**One Surprise:**  \n",
        "Training accuracy (70%) vs test accuracy (49%) shows massive overfitting - 21% gap! The model memorized training data but can't generalize. The confusion matrix revealed severe bias toward \"Characters\" class (77/132 predictions). Training was surprisingly fast but results were disappointing.\n",
        "\n",
        "**One Change for Next Time:**  \n",
        "Would implement regularization techniques (dropout, weight decay) earlier to combat overfitting. Also would track more metrics during training (per-class accuracy, F1-score) to catch bias issues sooner.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Entry 8: Notebook 07 - TinyBERT Setup\n",
        "\n",
        "**Date:** October 23, 2025  \n",
        "**Notebook:** 07_tinybert_setup_and_freeze.ipynb  \n",
        "\n",
        "**What Worked:**  \n",
        "Loading pre-trained TinyBERT was easy with `transformers` library. Understanding that `num_labels=3` sets output size made sense. Freezing layers with `param.requires_grad = False` was straightforward.\n",
        "\n",
        "**What Broke:**  \n",
        "Confusion about `num_labels` (is it input or output?), attention masks (why different from padding?), and layer freezing strategy (why layer 3?). Also unclear why we don't get catastrophic forgetting when unfreezing layers - seems risky!\n",
        "\n",
        "**One Surprise:**  \n",
        "TinyBERT has 4 layers vs BERT's 12 - it's really \"tiny\"! The pre-trained vocab (30K+ tokens) is WAY bigger than my custom vocab (1002 words). Attention masks are different from padding - they tell the model which tokens to \"pay attention to\". Strategic freezing (95% frozen, 5% trainable) prevents catastrophic forgetting.\n",
        "\n",
        "**One Change for Next Time:**  \n",
        "Would experiment with different freezing strategies (freeze nothing, freeze everything except classifier, freeze first 2 layers vs first 3 layers) to understand the trade-offs.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Entry 9: Notebook 08 - TinyBERT Fine-Tuning\n",
        "\n",
        "**Date:** October 23, 2025  \n",
        "**Notebook:** 08_tinybert_finetune_trainloop.ipynb  \n",
        "\n",
        "**What Worked:**  \n",
        "The training loop with early stopping worked beautifully! `lr=5e-4` was the \"Goldilocks zone\" - fast convergence without oscillation. The professional visualization (dual subplots with overfitting gap) made training behavior crystal clear.\n",
        "\n",
        "**What Broke:**  \n",
        "Initial `lr=1e-5` was too conservative (barely learned). `lr=2.5e-3` was too aggressive (oscillated wildly). Hit a `TypeError` comparing list vs float in early stopping logic. Confusion about \"patience\" mechanism and what \"New best!\" vs \"Patience\" messages meant.\n",
        "\n",
        "**One Surprise:**  \n",
        "Early stopping saved 24 epochs! Best model was at epoch 26, not epoch 50. Test loss dropped from 1.08 (baseline) to 0.1045 (TinyBERT) - a 90% reduction! The patience mechanism automatically restores the best checkpoint. AdamW optimizer with weight decay is specifically designed for transformers.\n",
        "\n",
        "**One Change for Next Time:**  \n",
        "Would try learning rate scheduling (start high, decay over time) to potentially squeeze out even better performance. Also would experiment with different patience values (3, 7, 10).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Entry 10: Notebook 09 - Evaluation & Comparison\n",
        "\n",
        "**Date:** October 23, 2025  \n",
        "**Notebook:** 09_tinybert_eval_compare.ipynb  \n",
        "\n",
        "**What Worked:**  \n",
        "Loading both models from checkpoints for fair comparison was straightforward. The comparison table clearly showed TinyBERT's superiority (94% vs 49% accuracy). Using `seaborn.heatmap` fixed the confusion matrix legend overlap issue.\n",
        "\n",
        "**What Broke:**  \n",
        "Initially tried importing `SimpleNN` from another notebook with `nbimporter` - didn't work! Had to copy the class definition directly. The manual confusion matrix plotting had overlapping legends. Confusion about why I was preprocessing data again (needed baseline model's vocab for fair comparison).\n",
        "\n",
        "**One Surprise:**  \n",
        "**94% accuracy with TinyBERT!** A 91% improvement over baseline. Story classification went from 36% to 98% recall (+173%!). The baseline's severe bias toward \"Characters\" completely disappeared with TinyBERT. Transfer learning is TRANSFORMATIVELY better than training from scratch.\n",
        "\n",
        "**One Change for Next Time:**  \n",
        "Would do error analysis on the 8 misclassified TinyBERT samples to understand failure modes. Also would try ensemble methods (multiple TinyBERT models with different seeds) to push accuracy even higher.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Final Reflection: Learning Patterns\n",
        "\n",
        "**At the end of your project, read all your entries and reflect on these questions:**\n",
        "\n",
        "### üìä Pattern Recognition\n",
        "- What themes do you notice across your learning journey?\n",
        "- Which concepts were consistently challenging?\n",
        "- Where did you experience the most \"aha\" moments?\n",
        "\n",
        "### üöÄ Growth Insights\n",
        "- How did your understanding of NLP evolve?\n",
        "- What skills did you develop that surprised you?\n",
        "- Which approach worked best for your learning style?\n",
        "\n",
        "### üîÆ Future Applications\n",
        "- How will you apply these concepts to new projects?\n",
        "- What would you do differently if starting over?\n",
        "- What are you excited to explore next?\n",
        "\n",
        "---\n",
        "\n",
        "**My Final Reflections:**\n",
        "\n",
        "### üìä Pattern Recognition\n",
        "\n",
        "**Recurring Themes:**\n",
        "1. **Confusion ‚Üí Clarity ‚Üí Confidence**: Almost every notebook started with confusion (vocab indices, tensor shapes, attention masks), followed by guided exploration, then eventual understanding. This pattern repeated consistently.\n",
        "\n",
        "2. **The Power of Visualization**: Every time I visualized something (confusion matrices, loss curves, overfitting gaps), my understanding deepened significantly. Seeing data >> hearing explanations.\n",
        "\n",
        "3. **Hands-On Debugging = Best Learning**: My deepest learning moments came from fixing bugs (FloatTensor error, early stopping TypeError, confusion matrix overlap). Debugging forced me to truly understand what each line does.\n",
        "\n",
        "**Consistently Challenging Concepts:**\n",
        "- **Tensor shape transformations** (3D ‚Üí 2D via mean pooling)\n",
        "- **Why certain design choices matter** (batch size, learning rate, vocab size)\n",
        "- **Regex patterns** (still don't love them, still rely on LLMs!)\n",
        "- **Reading confusion matrices** (which axis is predicted vs true?)\n",
        "\n",
        "**\"Aha!\" Moments:**\n",
        "1. **Notebook 03**: Realizing that limiting vocab to 1000 words means 30-50% of test words map to `<UNK>` - huge information loss!\n",
        "2. **Notebook 05**: Understanding embeddings as spatial representations where semantic meaning = mathematical distance ü§Ø\n",
        "3. **Notebook 06**: Seeing 70% train accuracy vs 49% test accuracy - overfitting visualized!\n",
        "4. **Notebook 08**: Discovering early stopping saved 24 epochs by automatically finding the best checkpoint\n",
        "5. **Notebook 09**: The 91% accuracy improvement with TinyBERT - transfer learning validation!\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Growth Insights\n",
        "\n",
        "**Evolution of NLP Understanding:**\n",
        "\n",
        "**Beginning (Notebooks 01-03):**\n",
        "- NLP = \"text ‚Üí numbers somehow\"\n",
        "- Thought tokenization was simple (just split on spaces!)\n",
        "- Didn't understand why vocab size mattered\n",
        "\n",
        "**Middle (Notebooks 04-06):**\n",
        "- Realized NLP pipeline complexity (tokenize ‚Üí encode ‚Üí pad ‚Üí embed ‚Üí pool ‚Üí classify)\n",
        "- Understood embeddings as learned spatial representations\n",
        "- Saw firsthand how small vocab + no context = poor performance (49% accuracy)\n",
        "\n",
        "**End (Notebooks 07-09):**\n",
        "- Grasped transfer learning paradigm: pre-training on millions of examples >> training from scratch\n",
        "- Understood attention mechanisms (context-aware vs simple embeddings)\n",
        "- Realized subword tokenization (30K tokens) >> word-level (1K words)\n",
        "- **Key insight**: For production NLP, always start with pre-trained transformers\n",
        "\n",
        "**Skills That Surprised Me:**\n",
        "1. **Hyperparameter tuning intuition**: Learning to \"feel\" when LR is too low (slow convergence) vs too high (oscillation)\n",
        "2. **Debugging PyTorch errors**: Tensor type mismatches, shape errors - I can read these now!\n",
        "3. **Interpreting metrics**: Confusion matrices, precision/recall trade-offs, overfitting gaps\n",
        "4. **Strategic layer freezing**: Understanding 95% frozen + 5% trainable prevents catastrophic forgetting\n",
        "\n",
        "**Best Learning Approach:**\n",
        "**Guided discovery with immediate feedback** worked best for me:\n",
        "- Attempt solution ‚Üí hit error ‚Üí understand why ‚Üí fix ‚Üí reflect\n",
        "- This beats pure lecture or pure trial-and-error\n",
        "- Having hints without complete solutions forced me to think\n",
        "\n",
        "---\n",
        "\n",
        "### üîÆ Future Applications\n",
        "\n",
        "**How I'll Apply These Concepts:**\n",
        "\n",
        "1. **For Any NLP Task:**\n",
        "   - Start with pre-trained transformer (TinyBERT, BERT, RoBERTa)\n",
        "   - Use strategic layer freezing (freeze most, train classifier + last layer)\n",
        "   - Employ early stopping with patience (don't waste epochs!)\n",
        "   - Always visualize: loss curves, confusion matrices, overfitting gaps\n",
        "   - Compare to simple baseline (appreciate improvements!)\n",
        "\n",
        "2. **For Small Datasets (<1000 samples):**\n",
        "   - Transfer learning is ESSENTIAL (can't learn from scratch)\n",
        "   - Freeze most layers to prevent overfitting\n",
        "   - Higher LR works when only 5% of model is trainable\n",
        "   - Data augmentation might help (back-translation, paraphrasing)\n",
        "\n",
        "3. **For Production Systems:**\n",
        "   - TinyBERT >> baseline for real users (94% vs 49%)\n",
        "   - Model size trade-off: TinyBERT (64MB) is acceptable for most apps\n",
        "   - Always track per-class metrics to catch bias (baseline: 77% predicted \"Characters\"!)\n",
        "\n",
        "**What I'd Do Differently:**\n",
        "\n",
        "1. **Spend more time on data exploration** (Notebook 01): Understanding the 3-class balance, review length distribution, vocab coverage upfront would have informed later decisions.\n",
        "\n",
        "2. **Experiment with vocab sizes earlier** (Notebook 03): Trying 500, 2000, 5000 word vocabs would have taught me the sweet spot faster.\n",
        "\n",
        "3. **Implement learning rate scheduling from the start** (Notebook 08): Start high (5e-4), decay to low (1e-4) might have reached 95%+ accuracy.\n",
        "\n",
        "4. **Do error analysis immediately** (Notebook 09): Manually reviewing the 8 misclassified TinyBERT samples could reveal systematic failures.\n",
        "\n",
        "5. **Track more metrics during training**: Per-class accuracy, F1-score, not just overall loss/accuracy - would have caught \"Characters\" bias sooner.\n",
        "\n",
        "**What I'm Excited to Explore:**\n",
        "\n",
        "1. **Larger Models**: BERT-Base (110M params) vs TinyBERT (14M params) - diminishing returns?\n",
        "2. **Different Domains**: Sentiment analysis, question-answering, named entity recognition\n",
        "3. **Multimodal Models**: Combining text + images (e.g., VisualBERT)\n",
        "4. **Efficient Fine-Tuning**: LoRA, QLoRA - update <1% of parameters!\n",
        "5. **Explainability**: Why did TinyBERT misclassify those 8 samples?\n",
        "6. **Ensemble Methods**: Combine 5 TinyBERT models trained with different seeds\n",
        "7. **Larger Datasets**: How does TinyBERT scale to 10K, 100K, 1M samples?\n",
        "\n",
        "---\n",
        "\n",
        "### üéì **The Big Picture**\n",
        "\n",
        "**What This Project Taught Me:**\n",
        "\n",
        "This wasn't just about getting 94% accuracy. It was about:\n",
        "- **Understanding the fundamentals**: Tokenization ‚Üí Embeddings ‚Üí Classification\n",
        "- **Appreciating modern NLP**: Transfer learning changed everything\n",
        "- **Learning to learn**: Debugging, visualizing, iterating\n",
        "- **Thinking like an ML engineer**: Hyperparameters, metrics, trade-offs\n",
        "\n",
        "**The Most Important Lesson:**\n",
        "\n",
        "**The era of building NLP from scratch is over for production.** Transfer learning with pre-trained transformers (TinyBERT, BERT, etc.) is:\n",
        "- 91% more accurate than baseline\n",
        "- 48% faster to train (early stopping)\n",
        "- More generalizable (6.6% vs 21% overfitting)\n",
        "- Handles any vocabulary (subword tokenization)\n",
        "\n",
        "But understanding the baseline pipeline (Notebooks 01-06) was ESSENTIAL to appreciate why transformers are so powerful. You can't understand the revolution without knowing what came before.\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ **Next Steps**\n",
        "\n",
        "1. **Short Term** (This Week):\n",
        "   - Error analysis on 8 misclassified TinyBERT samples\n",
        "   - Try unfreezing layer 2 as well (layers 2-3 + classifier)\n",
        "   - Experiment with learning rate scheduling\n",
        "\n",
        "2. **Medium Term** (This Month):\n",
        "   - Apply TinyBERT to a different dataset (e.g., product reviews)\n",
        "   - Try BERT-Base vs TinyBERT comparison\n",
        "   - Implement data augmentation (back-translation)\n",
        "\n",
        "3. **Long Term** (This Year):\n",
        "   - Explore multimodal models (text + images)\n",
        "   - Learn about LLMs (GPT-style models)\n",
        "   - Build a production NLP app (deployed API)\n",
        "\n",
        "**Final Thought:**\n",
        "\n",
        "I started this project confused about tokenization and vocab indices. I'm ending it with a working transformer model at 94% accuracy, a deep understanding of the NLP pipeline, and confidence to tackle any text classification task. That's genuine progress. üéâ\n",
        "\n",
        "**Thank you to this structured, didactic approach!** The TODO-style notebooks with hints (not solutions) forced me to think. The reflection prompts forced me to articulate understanding. The bug fixes forced me to truly learn. This is how ML education should be done. üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
