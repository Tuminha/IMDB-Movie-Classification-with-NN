{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧠 Notebook 05: Simple Neural Network with Embedding\n",
        "\n",
        "## Building Your First NLP Model\n",
        "\n",
        "This notebook teaches you how to build a neural network specifically designed for text classification. You'll create an embedding layer that learns dense representations of words, then use mean pooling to aggregate sequences into fixed-size vectors for classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 Concept Primer: Neural Network Architecture\n",
        "\n",
        "### What We're Building\n",
        "A neural network that transforms sequences of word IDs into aspect predictions through learned embeddings and mean pooling.\n",
        "\n",
        "### Why This Architecture Works\n",
        "**Embeddings learn semantic relationships.** Instead of treating words as discrete symbols, embeddings represent them as dense vectors where similar words have similar representations.\n",
        "\n",
        "### Architecture Flow\n",
        "1. **Input**: Word IDs `[batch, seq_len]` → `[16, 128]`\n",
        "2. **Embedding**: Lookup table → `[batch, seq_len, embed_dim]` → `[16, 128, 50]`\n",
        "3. **Masking**: Ignore padding tokens during pooling\n",
        "4. **Mean Pool**: Aggregate sequence → `[batch, embed_dim]` → `[16, 50]`\n",
        "5. **Linear Layers**: Classification → `[batch, n_classes]` → `[16, n_aspects]`\n",
        "\n",
        "### Math Mapping\n",
        "- **Embedding**: `nn.Embedding(vocab_size, embed_dim)` → lookup table\n",
        "- **Masking**: `(x != pad_id).unsqueeze(-1).float()` → `[batch, seq, 1]`\n",
        "- **Mean Pool**: `(embedded * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)`\n",
        "- **Classification**: `Linear(embed_dim, hidden) → ReLU → Linear(hidden, n_classes)`\n",
        "\n",
        "### Common Pitfalls\n",
        "- **Forgetting `.clamp(min=1)`** causes division by zero for empty sequences\n",
        "- **Wrong padding_idx** in embedding layer breaks gradient flow\n",
        "- **Incorrect tensor shapes** cause runtime errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 TODO #1: Define Model Class\n",
        "\n",
        "**Task:** Create the neural network class with embedding layer and linear layers.\n",
        "\n",
        "**Hint:** Use `class SimpleNNWithEmbedding(nn.Module):` with `__init__(self, vocab_size, embed_size=50, hidden_size=100, output_size, pad_id=1)`\n",
        "\n",
        "**Expected Class Structure:**\n",
        "```python\n",
        "class SimpleNNWithEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=50, hidden_size=100, output_size, pad_id=1):\n",
        "        super().__init__()\n",
        "        # TODO: Define layers here\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward pass\n",
        "        return logits\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #1: Define model class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Your code here\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size,embed_size=50, hidden_size=100, pad_id=0):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_id)\n",
        "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        embedded = self.embedding(x)  # [16, 128, 50]\n",
        "        mask = (x != self.pad_id).unsqueeze(-1).float()  # [16, 128, 1]\n",
        "        masked_embedded = embedded * mask  # [16, 128, 50]\n",
        "        pooled = masked_embedded.sum(dim=1) / mask.sum(dim=1).clamp(min=1)  # [16, 50]\n",
        "\n",
        "\n",
        "        x = self.fc1(pooled)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 TODO #2: Implement Forward Pass\n",
        "\n",
        "**Task:** Complete the forward method with embedding lookup, masking, mean pooling, and classification.\n",
        "\n",
        "**Shape Hints:**\n",
        "- `embedded = self.embedding(x)` → `[batch, seq, embed]`\n",
        "- `mask = (x != self.pad_id).unsqueeze(-1).float()` → `[batch, seq, 1]`\n",
        "- `masked_sum = (embedded * mask).sum(dim=1)` → `[batch, embed]`\n",
        "- `count = mask.sum(dim=1).clamp(min=1)`\n",
        "- `pooled = masked_sum / count` → `[batch, embed]`\n",
        "- Pass through Linear1+ReLU, then Linear2 → `[batch, n_aspects]`\n",
        "\n",
        "**Critical:** Don't forget `.clamp(min=1)` to prevent division by zero!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #2: Implement forward pass\n",
        "# Add your forward method implementation to the class above\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Your code here\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size,embed_size=50, hidden_size=100, pad_id=0):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_id)\n",
        "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        embedded = self.embedding(x)  # [16, 128, 50]\n",
        "        mask = (x != self.pad_id).unsqueeze(-1).float()  # [16, 128, 1]\n",
        "        masked_embedded = embedded * mask  # [16, 128, 50]\n",
        "        pooled = masked_embedded.sum(dim=1) / mask.sum(dim=1).clamp(min=1)  # [16, 50]\n",
        "\n",
        "\n",
        "        x = self.fc1(pooled)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 TODO #3: Instantiate Model and Training Components\n",
        "\n",
        "**Task:** Create model instance, loss function, and optimizer.\n",
        "\n",
        "**Hint:** \n",
        "- `model = SimpleNNWithEmbedding(vocab_size=len(vocab), output_size=n_aspects)`\n",
        "- `criterion = nn.CrossEntropyLoss()`\n",
        "- `optimizer = torch.optim.Adam(model.parameters(), lr=0.005)`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `model` → Instantiated neural network\n",
        "- `criterion` → Cross-entropy loss function\n",
        "- `optimizer` → Adam optimizer with learning rate 0.005\n",
        "\n",
        "**Test:** Try `model(X_batch)` to verify forward pass returns shape `[16, n_aspects]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('the', 1009), ('a', 423), ('of', 405), ('and', 399), ('to', 295), ('is', 295), ('in', 235), ('it', 191), ('s', 147), ('that', 140)]\n",
            "[('<PAD>', 0), ('<UNK>', 1), ('the', 2), ('a', 3), ('of', 4), ('and', 5), ('to', 6), ('is', 7), ('in', 8), ('it', 9)]\n"
          ]
        }
      ],
      "source": [
        "# Comes from the previous notebooks\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def pad_or_truncate(sequence, max_len, padding_value=0):\n",
        "    \"\"\"\n",
        "    Pads or truncates a sequence to a specified target length.\n",
        "\n",
        "    Parameters:\n",
        "    sequence (list): The input sequence to be padded or truncated.\n",
        "    target_length (int): The desired length of the output sequence.\n",
        "    padding_value (any): The value to use for padding if the sequence is shorter than the target length.\n",
        "\n",
        "    Returns:\n",
        "    list: The padded or truncated sequence.\n",
        "    \"\"\"\n",
        "    if len(sequence) < max_len:\n",
        "        # Pad the sequence\n",
        "        return sequence + [padding_value] * (max_len - len(sequence))\n",
        "    else:\n",
        "        # Truncate the sequence\n",
        "        return sequence[:max_len]\n",
        "\n",
        "# Test the function\n",
        "sample_sequence = [1, 2, 3]\n",
        "padded_sequence = pad_or_truncate(sample_sequence, 5, padding_value=0)\n",
        "truncated_sequence = pad_or_truncate(sample_sequence, 2)\n",
        "\n",
        "def tokenize(text):\n",
        "    # Use regex to find words, ignoring punctuation\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "train_reviews_df = pd.read_csv('../data/imdb_movie_reviews_train.csv')\n",
        "test_reviews_df = pd.read_csv('../data/imdb_movie_reviews_test.csv')\n",
        "\n",
        "tokenized_corpus_train = train_reviews_df['review'].apply(tokenize).tolist() \n",
        "tokenized_corpus_test = test_reviews_df['review'].apply(tokenize).tolist()\n",
        "\n",
        "combined_corpus = [token for sublist in tokenized_corpus_train + tokenized_corpus_test for token in sublist]\n",
        "\n",
        "word_freqs = Counter(combined_corpus)\n",
        "print(word_freqs.most_common(10))\n",
        "\n",
        "max_vocab_size = 1002\n",
        "most_common_words = word_freqs.most_common(max_vocab_size - 2)  # Reserve 2 for <PAD> and <UNK>\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1, **{word: idx + 2 for idx, (word, _) in enumerate(most_common_words)}}\n",
        "print(list(vocab.items())[:10])  # Print first 10 items in vocabulary dictionary\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def encode_text(text, vocab):\n",
        "    tokens = tokenize(text)\n",
        "    encoded = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "    return encoded\n",
        "\n",
        "encoded_reviews_train = train_reviews_df['review'].apply(lambda x: encode_text(x, vocab)).tolist()\n",
        "encoded_reviews_test = test_reviews_df['review'].apply(lambda x: encode_text(x, vocab)).tolist()\n",
        "\n",
        "max_sequence_length = 128\n",
        "padded_encoded_reviews_train = [pad_or_truncate(seq, max_sequence_length, padding_value=vocab['<PAD>']) for seq in encoded_reviews_train]\n",
        "padded_encoded_reviews_test = [pad_or_truncate(seq, max_sequence_length, padding_value=vocab['<PAD>']) for seq in encoded_reviews_test]\n",
        "X_tensor_train = torch.tensor(padded_encoded_reviews_train, dtype=torch.long)\n",
        "X_tensor_test = torch.tensor(padded_encoded_reviews_test, dtype=torch.long)\n",
        "y_tensor_train = torch.tensor(train_reviews_df['aspect_encoded'].values, dtype=torch.long)\n",
        "y_tensor_test = torch.tensor(test_reviews_df['aspect_encoded'].values, dtype=torch.long)\n",
        "\n",
        "batch_size = 16\n",
        "train_dataset = TensorDataset(X_tensor_train, y_tensor_train)\n",
        "test_dataset = TensorDataset(X_tensor_test, y_tensor_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #3: Instantiate model and training components\n",
        "# Your code here\n",
        "n_aspects = 3\n",
        "\n",
        "model = SimpleNN(vocab_size=len(vocab), output_size=n_aspects)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📝 Reflection Prompts\n",
        "\n",
        "### 🤔 Understanding Check\n",
        "1. **Why mask padding tokens in mean pooling?** What would happen if you included padding in the average?\n",
        "\n",
        "2. **What does the embedding layer actually learn?** How does it differ from one-hot encoding?\n",
        "\n",
        "3. **Why use mean pooling instead of just taking the last token?** What information would you lose?\n",
        "\n",
        "4. **How does the embedding dimension (50) affect model capacity?** What's the tradeoff?\n",
        "\n",
        "### 🎯 Architecture Design\n",
        "- Why is this architecture well-suited for text classification?\n",
        "- How does mean pooling handle variable-length sequences?\n",
        "- What would happen if you used max pooling instead of mean pooling?\n",
        "\n",
        "## 📝 Reflection Prompts\n",
        "\n",
        "### 🤔 Understanding Check\n",
        "1. **Why mask padding tokens in mean pooling?** What would happen if you included padding in the average?\n",
        "\n",
        "2. **What does the embedding layer actually learn?** How does it differ from one-hot encoding?\n",
        "\n",
        "3. **Why use mean pooling instead of just taking the last token?** What information would you lose?\n",
        "\n",
        "4. **How does the embedding dimension (50) affect model capacity?** What's the tradeoff?\n",
        "\n",
        "### 🎯 Architecture Design\n",
        "- Why is this architecture well-suited for text classification?\n",
        "- How does mean pooling handle variable-length sequences?\n",
        "- What would happen if you used max pooling instead of mean pooling?\n",
        "\n",
        "---\n",
        "\n",
        "## 📝 My Reflections\n",
        "\n",
        "### 🤔 Understanding Check Answers\n",
        "\n",
        "1. **Why mask padding tokens in mean pooling?**\n",
        "   - Masking prevents padding tokens from contributing to the average\n",
        "   - Without masking, we'd average real tokens + padding tokens, diluting the semantic meaning\n",
        "   - Padding tokens (0s) should be excluded from the mean calculation to preserve the true representation\n",
        "\n",
        "2. **What does the embedding layer actually learn?**\n",
        "   - The embedding layer learns the ideal spatial positions that words should occupy based on their semantic meaning and proximity\n",
        "   - Words with similar meanings get closer spatial positions in the 50-dimensional space\n",
        "   - Meaning becomes a mathematical formula where spatial differences translate into semantic relationships\n",
        "   - The 50 dimensions represent 50 potential spatial positions per token that can be related to other tokens\n",
        "\n",
        "3. **Why use mean pooling instead of just taking the last token?**\n",
        "   - **Mean pooling**: Preserves information from all words, creating a comprehensive summary of the entire review\n",
        "   - **Last token only**: Would lose information from all other words in the sequence\n",
        "   - Mean pooling handles variable length sequences by averaging across all tokens\n",
        "   - Taking only the last token would create huge discrepancies and lose semantic richness\n",
        "\n",
        "4. **How does the embedding dimension (50) affect model capacity?**\n",
        "   - More embeddings don't necessarily mean more accuracy - it's a computational trade-off\n",
        "   - 50 dimensions provide a good balance between representation capacity and computational efficiency\n",
        "   - Higher dimensions require more computational power for gradient calculations during training\n",
        "   - The key is finding the right balance for the specific task\n",
        "\n",
        "### 🎯 Architecture Design Analysis\n",
        "\n",
        "**Why this architecture is well-suited for text classification:**\n",
        "- Manages embeddings, handles shape differences between layers, and enables multi-class prediction\n",
        "- The flow: Word IDs → Embeddings → Masking → Pooling → Classification\n",
        "- Each step serves a specific purpose in converting text to predictions\n",
        "\n",
        "**How mean pooling handles variable-length sequences:**\n",
        "- Mean pooling aggregates the entire sequence into a fixed-size vector\n",
        "- Handles variable length sequences by averaging across all tokens\n",
        "- Preserves semantic information from all words rather than just the strongest\n",
        "- Creates a \"summary\" representation of the entire review\n",
        "\n",
        "**What would happen with max pooling instead:**\n",
        "- Max pooling would only consider the strongest/most important word\n",
        "- Would lose information from other words in the sequence\n",
        "- Could create huge discrepancies and lose the semantic richness of the text\n",
        "- Would not provide a comprehensive representation of the entire review\n",
        "\n",
        "**Key insight:** This architecture elegantly handles the fundamental challenge of converting variable-length text sequences into fixed-size representations that neural networks can process, while preserving semantic meaning through learned embeddings and intelligent pooling strategies.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
