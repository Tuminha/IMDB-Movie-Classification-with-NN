{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎬 IMDB Movie Reviews — Learning Journey Overview\n",
        "\n",
        "## Your NLP Learning Adventure\n",
        "\n",
        "Welcome to a **hands-on journey** from basic text processing to transformer fine-tuning. This repository teaches you NLP by building every component yourself, then comparing your baseline with TinyBERT.\n",
        "\n",
        "**Learning Philosophy:** Explain first, code later. Each notebook starts with concepts, then guides you through implementation with hints—no complete solutions provided.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 Concept Primer: The Complete NLP Pipeline\n",
        "\n",
        "### What We're Building\n",
        "A complete text classification pipeline that transforms raw movie reviews into aspect predictions, comparing two approaches:\n",
        "1. **Baseline**: Hand-crafted neural network with embeddings\n",
        "2. **TinyBERT**: Pre-trained transformer fine-tuned for the task\n",
        "\n",
        "### Why This Approach?\n",
        "Understanding each step prevents you from treating NLP as a \"black box.\" When something breaks (and it will!), you'll know exactly where to look.\n",
        "\n",
        "### The 9-Step Pipeline\n",
        "\n",
        "**Phase 1: Data Preparation (Notebooks 01-04)**\n",
        "1. **Import & Inspect** → Load CSVs, understand the data structure\n",
        "2. **Tokenization** → Split text into processable word units  \n",
        "3. **Vocabulary Building** → Create word→integer mappings\n",
        "4. **Padding & Tensors** → Convert to fixed-size tensors for neural networks\n",
        "\n",
        "**Phase 2: Baseline Model (Notebooks 05-06)**\n",
        "5. **Neural Network** → Embedding layer + mean pooling + classifier\n",
        "6. **Training & Evaluation** → Train loop + metrics computation\n",
        "\n",
        "**Phase 3: Transformer Comparison (Notebooks 07-09)**\n",
        "7. **TinyBERT Setup** → Load pre-trained model, freeze/unfreeze layers\n",
        "8. **Fine-tuning** → Adapt transformer to our specific task\n",
        "9. **Comparison** → Side-by-side performance analysis\n",
        "\n",
        "### How It Maps to Math\n",
        "- **Text → Tokens**: String splitting with regex\n",
        "- **Tokens → Integers**: Dictionary lookup (vocab mapping)\n",
        "- **Integers → Embeddings**: Lookup table → dense vectors\n",
        "- **Embeddings → Features**: Mean pooling → fixed-size representation\n",
        "- **Features → Predictions**: Linear layers → class probabilities\n",
        "- **Predictions → Loss**: Cross-entropy → optimization signal\n",
        "\n",
        "### Where It Maps to PyTorch Objects\n",
        "- `torch.nn.Embedding` → word ID lookup table\n",
        "- `torch.nn.Linear` → weight matrices for classification\n",
        "- `torch.nn.CrossEntropyLoss` → loss computation\n",
        "- `torch.optim.Adam` → gradient descent optimizer\n",
        "- `torch.utils.data.DataLoader` → batch management\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📋 Checklist Objectives\n",
        "\n",
        "By the end of this overview, you should be able to:\n",
        "\n",
        "- [ ] **Narrate the pipeline** in 60-90 seconds without looking at notes\n",
        "- [ ] **Explain why each step exists** and what breaks without it\n",
        "- [ ] **Identify the two main approaches** (baseline vs transformer)\n",
        "- [ ] **Understand the learning philosophy** (explain-first, code-later)\n",
        "\n",
        "## ✅ Acceptance Criteria\n",
        "\n",
        "**You've mastered this notebook when you can:**\n",
        "- Recite the 9-step pipeline from memory\n",
        "- Explain the difference between baseline and TinyBERT approaches\n",
        "- Articulate why we build from scratch before using pre-trained models\n",
        "- Feel confident about the learning journey ahead\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Pipeline Visual Diagram\n",
        "\n",
        "```\n",
        "Raw Text: \"This movie was amazing!\"\n",
        "    ↓\n",
        "[01] Load Data → DataFrame with columns: review, aspect, aspect_encoded\n",
        "    ↓\n",
        "[02] Tokenize → [\"this\", \"movie\", \"was\", \"amazing\"]\n",
        "    ↓\n",
        "[03] Build Vocab → {\"this\": 2, \"movie\": 3, \"was\": 4, \"amazing\": 5, \"<unk>\": 0, \"<pad>\": 1}\n",
        "    ↓\n",
        "[04] Encode & Pad → [2, 3, 4, 5, 1, 1, 1, ...] (length 128)\n",
        "    ↓\n",
        "[05] Neural Network → Embedding(50) → Mean Pool → Linear(100) → Linear(n_classes)\n",
        "    ↓\n",
        "[06] Train & Evaluate → Loss optimization → Accuracy/F1 metrics\n",
        "    ↓\n",
        "[07] TinyBERT Setup → Load pre-trained → Freeze most layers → Unfreeze classifier\n",
        "    ↓\n",
        "[08] Fine-tune → High LR (2.5e-3) → Adapt to task → Track loss\n",
        "    ↓\n",
        "[09] Compare → Baseline vs TinyBERT → Performance analysis → Insights\n",
        "```\n",
        "\n",
        "**Key Insight:** Notice how both paths (baseline and TinyBERT) converge at the same evaluation metrics, but they take very different routes to get there!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📝 Reflection Prompts\n",
        "\n",
        "Take 5-10 minutes to reflect on these questions. Write your thoughts in the cell below:\n",
        "\n",
        "### 🤔 Understanding Check\n",
        "1. **In your own words, why do we need a vocabulary?** What would happen if we tried to feed raw text directly to a neural network?\n",
        "\n",
        "2. **What's the fundamental difference between the baseline encoder and TinyBERT?** Think about what each approach \"knows\" before training starts.\n",
        "\n",
        "3. **Why do we build from scratch first, then use pre-trained models?** What would you miss if you jumped straight to TinyBERT?\n",
        "\n",
        "4. **Looking at the pipeline diagram above, which step seems most mysterious to you right now?** What would you like to understand better?\n",
        "\n",
        "### 🎯 Personal Learning Goals\n",
        "- What aspect of NLP are you most excited to learn about?\n",
        "- What's your biggest concern about this learning journey?\n",
        "- How do you plan to use these notebooks to build understanding (not just copy code)?\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflections here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 🚀 Ready to Begin?\n",
        "\n",
        "You're now equipped with the mental model for the entire NLP pipeline. When you're ready to start building, move on to **Notebook 01: Import and Inspect** where you'll load your first dataset and begin the hands-on journey.\n",
        "\n",
        "**Remember:** Each notebook builds on the previous one. Don't skip ahead—the learning is in the journey, not just the destination.\n",
        "\n",
        "**Next up:** Data familiarization and the first glimpse of your movie reviews dataset! 🎬\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
