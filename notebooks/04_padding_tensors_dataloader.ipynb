{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔢 Notebook 04: Padding, Tensors & DataLoader\n",
        "\n",
        "## From Variable Sequences to Fixed-Size Batches\n",
        "\n",
        "This notebook teaches you how to handle the fundamental challenge of neural networks: they need fixed-size inputs, but text sequences vary in length. You'll implement padding, convert to PyTorch tensors, and create efficient batch loading.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 Concept Primer: Padding and Batching\n",
        "\n",
        "### What We're Doing\n",
        "Converting variable-length encoded sequences into fixed-size tensors that neural networks can process in batches.\n",
        "\n",
        "### Why This Step is Critical\n",
        "**Neural networks require fixed-size inputs.** The challenges:\n",
        "- **Variable sequence lengths** (reviews have different numbers of words)\n",
        "- **Batch processing** (neural networks are faster with batches)\n",
        "- **Memory efficiency** (tensors must be rectangular)\n",
        "\n",
        "### What We'll Build\n",
        "- **Padding function** that extends short sequences with `<pad>` tokens\n",
        "- **Truncation logic** that cuts long sequences to max length\n",
        "- **Tensor conversion** using `torch.tensor()`\n",
        "- **DataLoader setup** for efficient batch loading\n",
        "\n",
        "### Shape Expectations\n",
        "- **Input sequences**: Variable length → Fixed length (128)\n",
        "- **Tensors**: `[batch_size, sequence_length]` → `[16, 128]`\n",
        "- **Labels**: `[batch_size]` → `[16]`\n",
        "\n",
        "### Expected Output Example\n",
        "```python\n",
        "for X_batch, y_batch in train_dataloader:\n",
        "    print(X_batch.shape, y_batch.shape)\n",
        "# Output: torch.Size([16, 128]) torch.Size([16])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 TODO #1: Implement Padding Function\n",
        "\n",
        "**Task:** Create function that pads short sequences and truncates long ones to fixed length.\n",
        "\n",
        "**Hint:** If `len(seq) < max_len`, extend with `1` (pad token); if longer, slice `seq[:max_len]`\n",
        "\n",
        "**Expected Function Signature:**\n",
        "```python\n",
        "def pad_or_truncate(seq, max_len=128):\n",
        "    # Your implementation here\n",
        "    return padded_seq  # List of length max_len\n",
        "```\n",
        "\n",
        "**Expected Output Example:**\n",
        "```python\n",
        "pad_or_truncate([2, 3, 4], max_len=5)\n",
        "# Returns: [2, 3, 4, 1, 1]  # padded\n",
        "\n",
        "pad_or_truncate([2, 3, 4, 5, 6, 7, 8], max_len=5)\n",
        "# Returns: [2, 3, 4, 5, 6]  # truncated\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padded Sequence: [1, 2, 3, 0, 0]\n",
            "Truncated Sequence: [1, 2]\n"
          ]
        }
      ],
      "source": [
        "# TODO #1: Implement padding function\n",
        "# Your code here\n",
        "from matplotlib.cm import ScalarMappable\n",
        "\n",
        "\n",
        "def pad_or_truncate(sequence, max_len, padding_value=0):\n",
        "    \"\"\"\n",
        "    Pads or truncates a sequence to a specified target length.\n",
        "\n",
        "    Parameters:\n",
        "    sequence (list): The input sequence to be padded or truncated.\n",
        "    target_length (int): The desired length of the output sequence.\n",
        "    padding_value (any): The value to use for padding if the sequence is shorter than the target length.\n",
        "\n",
        "    Returns:\n",
        "    list: The padded or truncated sequence.\n",
        "    \"\"\"\n",
        "    if len(sequence) < max_len:\n",
        "        # Pad the sequence\n",
        "        return sequence + [padding_value] * (max_len - len(sequence))\n",
        "    else:\n",
        "        # Truncate the sequence\n",
        "        return sequence[:max_len]\n",
        "\n",
        "# Test the function\n",
        "sample_sequence = [1, 2, 3]\n",
        "padded_sequence = pad_or_truncate(sample_sequence, 5, padding_value=0)\n",
        "truncated_sequence = pad_or_truncate(sample_sequence, 2)\n",
        "print(\"Padded Sequence:\", padded_sequence)        # Output: [1, 2, 3, 0, 0]\n",
        "print(\"Truncated Sequence:\", truncated_sequence)  # Output: [1, 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 TODO #2: Encode and Pad All Texts\n",
        "\n",
        "**Task:** Apply encoding and padding to all training texts.\n",
        "\n",
        "**Hint:** Use list comprehension: `padded_text_seqs = [pad_or_truncate(encode_text(text, vocab)) for text in train_texts]`\n",
        "\n",
        "**Expected Variable:**\n",
        "- `padded_text_seqs` → List of lists, each inner list has length 128\n",
        "\n",
        "**Shape Check:** All sequences should have the same length (128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('the', 1009), ('a', 423), ('of', 405), ('and', 399), ('to', 295), ('is', 295), ('in', 235), ('it', 191), ('s', 147), ('that', 140)]\n",
            "[('<PAD>', 0), ('<UNK>', 1), ('the', 2), ('a', 3), ('of', 4), ('and', 5), ('to', 6), ('is', 7), ('in', 8), ('it', 9)]\n",
            "First encoded and padded review (train): [1, 242, 536, 131, 27, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "First encoded and padded review (test): [2, 66, 7, 1, 9, 36, 292, 135, 136, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Checking that all sequences are of length 128\n",
            "Train lengths: [128, 128, 128, 128, 128]\n",
            "Test lengths: [128, 128, 128, 128, 128]\n",
            "Train lengths (last 5): [128, 128, 128, 128, 128]\n"
          ]
        }
      ],
      "source": [
        "# TODO #2: Encode and pad all texts\n",
        "# Your code here\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    # Use regex to find words, ignoring punctuation\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "train_reviews_df = pd.read_csv('../data/imdb_movie_reviews_train.csv')\n",
        "test_reviews_df = pd.read_csv('../data/imdb_movie_reviews_test.csv')\n",
        "\n",
        "tokenized_corpus_train = train_reviews_df['review'].apply(tokenize).tolist() \n",
        "tokenized_corpus_test = test_reviews_df['review'].apply(tokenize).tolist()\n",
        "\n",
        "combined_corpus = [token for sublist in tokenized_corpus_train + tokenized_corpus_test for token in sublist]\n",
        "\n",
        "word_freqs = Counter(combined_corpus)\n",
        "print(word_freqs.most_common(10))\n",
        "\n",
        "max_vocab_size = 1002\n",
        "most_common_words = word_freqs.most_common(max_vocab_size - 2)  # Reserve 2 for <PAD> and <UNK>\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1, **{word: idx + 2 for idx, (word, _) in enumerate(most_common_words)}}\n",
        "print(list(vocab.items())[:10])  # Print first 10 items in vocabulary dictionary\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def encode_text(text, vocab):\n",
        "    tokens = tokenize(text)\n",
        "    encoded = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "    return encoded\n",
        "\n",
        "encoded_reviews_train = train_reviews_df['review'].apply(lambda x: encode_text(x, vocab)).tolist()\n",
        "encoded_reviews_test = test_reviews_df['review'].apply(lambda x: encode_text(x, vocab)).tolist()\n",
        "\n",
        "max_sequence_length = 128\n",
        "padded_encoded_reviews_train = [pad_or_truncate(seq, max_sequence_length, padding_value=vocab['<PAD>']) for seq in encoded_reviews_train]\n",
        "padded_encoded_reviews_test = [pad_or_truncate(seq, max_sequence_length, padding_value=vocab['<PAD>']) for seq in encoded_reviews_test]\n",
        "print(\"First encoded and padded review (train):\", padded_encoded_reviews_train[0])\n",
        "print(\"First encoded and padded review (test):\", padded_encoded_reviews_test[0])\n",
        "print(\"Checking that all sequences are of length\", max_sequence_length)\n",
        "print(\"Train lengths:\", [len(seq) for seq in padded_encoded_reviews_train][:5])\n",
        "print(\"Test lengths:\", [len(seq) for seq in padded_encoded_reviews_test][:5])\n",
        "print(\"Train lengths (last 5):\", [len(seq) for seq in padded_encoded_reviews_train][-5:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 TODO #3: Convert to PyTorch Tensors\n",
        "\n",
        "**Task:** Convert padded sequences and labels to PyTorch tensors.\n",
        "\n",
        "**Hint:** Use `torch.tensor(padded_text_seqs, dtype=torch.long)` and `torch.tensor(train_labels, dtype=torch.long)`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `X_tensor` → Tensor of shape `[num_samples, 128]`\n",
        "- `y_tensor` → Tensor of shape `[num_samples]`\n",
        "\n",
        "**Data Types:** Use `torch.long` for both (integers for embeddings and class labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_tensor_train shape: torch.Size([369, 128])\n",
            "y_tensor_train shape: torch.Size([369])\n",
            "X_tensor_test shape: torch.Size([132, 128])\n",
            "y_tensor_test shape: torch.Size([132])\n"
          ]
        }
      ],
      "source": [
        "# TODO #3: Convert to PyTorch tensors\n",
        "import torch\n",
        "\n",
        "# Your code here\n",
        "X_tensor_train = torch.tensor(padded_encoded_reviews_train, dtype=torch.long)\n",
        "X_tensor_test = torch.tensor(padded_encoded_reviews_test, dtype=torch.long)\n",
        "y_tensor_train = torch.tensor(train_reviews_df['aspect_encoded'].values, dtype=torch.long)\n",
        "y_tensor_test = torch.tensor(test_reviews_df['aspect_encoded'].values, dtype=torch.long)\n",
        "print(\"X_tensor_train shape:\", X_tensor_train.shape)\n",
        "print(\"y_tensor_train shape:\", y_tensor_train.shape)\n",
        "print(\"X_tensor_test shape:\", X_tensor_test.shape)\n",
        "print(\"y_tensor_test shape:\", y_tensor_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 TODO #4: Create DataLoader\n",
        "\n",
        "**Task:** Build PyTorch DataLoader for efficient batch processing.\n",
        "\n",
        "**Hint:** Use `TensorDataset(X_tensor, y_tensor)` then `DataLoader(train_dataset, batch_size=16, shuffle=True)`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `train_dataset` → TensorDataset combining features and labels\n",
        "- `train_dataloader` → DataLoader with batch size 16 and shuffling\n",
        "\n",
        "**Expected Output:** When you iterate through the DataLoader, you should get batches of shape `[16, 128]` and `[16]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of batches in train_loader: 24\n",
            "Number of batches in test_loader: 9\n",
            "X_batch shape: torch.Size([16, 128])\n",
            "y_batch shape: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# TODO #4: Create DataLoader\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Your code here\n",
        "batch_size = 16\n",
        "train_dataset = TensorDataset(X_tensor_train, y_tensor_train)\n",
        "test_dataset = TensorDataset(X_tensor_test, y_tensor_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "print(\"Number of batches in train_loader:\", len(train_loader))\n",
        "print(\"Number of batches in test_loader:\", len(test_loader))\n",
        "#Shape of first batch\n",
        "for X_batch, y_batch in train_loader:\n",
        "    print(\"X_batch shape:\", X_batch.shape)\n",
        "    print(\"y_batch shape:\", y_batch.shape)\n",
        "    break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📝 Reflection Prompts\n",
        "\n",
        "### 🤔 Understanding Check\n",
        "1. **Why shuffle training data?** What would happen if you didn't shuffle?\n",
        "\n",
        "2. **What's the tradeoff of max_len=128 vs 256?** Consider memory usage and information loss.\n",
        "\n",
        "3. **Why use `torch.long` for both inputs and labels?** What would happen with other data types?\n",
        "\n",
        "4. **How does padding affect the model's understanding?** Will the model \"see\" the padding tokens?\n",
        "\n",
        "### 🎯 Batching Strategy\n",
        "- Why is batch processing more efficient than processing one sample at a time?\n",
        "- How does the batch size of 16 affect training speed vs memory usage?\n",
        "- What happens if your dataset size isn't divisible by batch size?\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflections here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📝 My Reflections\n",
        "\n",
        "### 🤔 Understanding Check Answers\n",
        "\n",
        "1. **Why shuffle training data?**\n",
        "   - Shuffling prevents the neural network from learning the sequence order of the data\n",
        "   - This randomness adds difficulty to the NN, forcing it to learn patterns rather than memorizing data order\n",
        "   - Without shuffling, the model might overfit to the specific order of samples in the dataset\n",
        "\n",
        "2. **What's the tradeoff of max_len=128 vs 256?**\n",
        "   - **Shorter sequences (128)**: Less memory usage, more computationally efficient, but potential information loss\n",
        "   - **Longer sequences (256)**: More information preserved, but higher memory usage and slower computation\n",
        "   - The trade-off balances computational efficiency with information retention\n",
        "\n",
        "3. **Why use `torch.long` for both inputs and labels?**\n",
        "   - `torch.long` represents 64-bit integers, which are needed for:\n",
        "     - **Inputs**: Word indices for embedding lookup (must be integers)\n",
        "     - **Labels**: Class indices for classification (must be integers)\n",
        "   - Other data types like `float` would be inappropriate for discrete indices\n",
        "\n",
        "4. **How does padding affect the model's understanding?**\n",
        "   - The model will see padding tokens (0s) and mathematically learn to ignore them\n",
        "   - The embedding layer and masking mechanisms help the model distinguish between real tokens and padding\n",
        "   - The model learns to focus on actual content while treating padding as neutral information\n",
        "\n",
        "### 🎯 Batching Strategy Analysis\n",
        "\n",
        "**Why batch processing is more efficient:**\n",
        "- Processing one sample at a time would interrupt the learning phase, making it less smooth\n",
        "- Batches allow for parallel computation and more stable gradient updates\n",
        "- A batch size of 16 is something a CPU can handle and avoids crashes\n",
        "\n",
        "**Batch size considerations:**\n",
        "- Batch size of 16 balances training speed with memory usage\n",
        "- The dataset should ideally be divisible by batch size to avoid incomplete batches\n",
        "- Larger batches provide more stable gradients but require more memory\n",
        "\n",
        "**Key insight:** Batching transforms individual samples into efficient parallel processing units, enabling smooth gradient-based learning while maintaining computational feasibility.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
