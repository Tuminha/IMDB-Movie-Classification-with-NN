{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Notebook 08: TinyBERT Fine-Tuning Training Loop\n",
        "\n",
        "## Adapting Pre-Trained Knowledge\n",
        "\n",
        "This notebook teaches you how to fine-tune TinyBERT using a high learning rate (2.5e-3) to observe the effects of aggressive learning on pre-trained models. You'll implement the training loop with attention masks and track the model's adaptation to your specific task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Concept Primer: Fine-Tuning with High Learning Rate\n",
        "\n",
        "### What We're Doing\n",
        "Fine-tuning TinyBERT on your specific task using an experimental high learning rate (2.5e-3) to observe the effects of aggressive learning on pre-trained models.\n",
        "\n",
        "### Why This Learning Rate is Experimental\n",
        "**Standard transformer fine-tuning uses ~2e-5.** Our 2.5e-3 is 100x higher! This will help you observe:\n",
        "- **Loss oscillation** vs smooth decrease\n",
        "- **Overfitting risk** with high LR\n",
        "- **Pre-trained knowledge retention** under aggressive updates\n",
        "\n",
        "### Training Loop Differences\n",
        "- **Attention masks** must be passed to the model\n",
        "- **Labels** can be passed directly to the model (it computes loss internally)\n",
        "- **Unfrozen parameters only** are updated by the optimizer\n",
        "\n",
        "### Expected Behavior\n",
        "- **Smooth decrease**: Model adapts well to task\n",
        "- **Oscillation**: Learning rate too high, causing instability\n",
        "- **Plateau**: Model has reached local minimum\n",
        "\n",
        "### Common Pitfalls\n",
        "- **Passing labels separately** instead of inside `model()` call\n",
        "- **Forgetting attention masks** breaks attention mechanism\n",
        "- **Not tracking unfrozen parameters** updates frozen layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #1: Create Optimizer for Unfrozen Parameters\n",
        "\n",
        "**Task:** Build AdamW optimizer that only updates unfrozen parameters.\n",
        "\n",
        "**Hint:** Use `optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=2.5e-3, weight_decay=0.01)`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `optimizer` ‚Üí AdamW optimizer with high learning rate and weight decay\n",
        "\n",
        "**Key Parameters:**\n",
        "- `lr=2.5e-3` ‚Üí High learning rate (experimental)\n",
        "- `weight_decay=0.01` ‚Üí L2 regularization to prevent overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #1: Create optimizer for unfrozen parameters\n",
        "import torch\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #2: Implement Fine-Tuning Training Loop\n",
        "\n",
        "**Task:** Create training loop that fine-tunes TinyBERT with attention masks.\n",
        "\n",
        "**Hint:** Use `model.train()`, unpack batch as `input_ids, attention_mask, labels = batch`, forward with `outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)`, then `loss = outputs.loss`\n",
        "\n",
        "**Expected Function:**\n",
        "```python\n",
        "def train_tinybert(model, train_loader, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        # TODO: Training loop here\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "```\n",
        "\n",
        "**Track:** Loss per epoch to observe training behavior\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #2: Implement fine-tuning training loop\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Reflection Prompts\n",
        "\n",
        "### ü§î Understanding Check\n",
        "1. **Did loss decrease smoothly or jump around?** What does this tell you about the learning rate?\n",
        "\n",
        "2. **Compare this LR to typical transformer LRs (2e-5)‚Äîwhat's the effect?** How does this impact training stability?\n",
        "\n",
        "3. **Why use AdamW instead of Adam?** What does weight decay accomplish?\n",
        "\n",
        "4. **How does passing labels to the model differ from computing loss separately?** What are the benefits?\n",
        "\n",
        "### üéØ Training Behavior Analysis\n",
        "- Was the high learning rate beneficial or harmful?\n",
        "- How did the loss curve compare to your baseline model?\n",
        "- What would you expect with a lower learning rate?\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflections here:**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
