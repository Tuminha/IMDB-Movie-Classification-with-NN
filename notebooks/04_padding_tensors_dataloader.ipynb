{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”¢ Notebook 04: Padding, Tensors & DataLoader\n",
        "\n",
        "## From Variable Sequences to Fixed-Size Batches\n",
        "\n",
        "This notebook teaches you how to handle the fundamental challenge of neural networks: they need fixed-size inputs, but text sequences vary in length. You'll implement padding, convert to PyTorch tensors, and create efficient batch loading.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§  Concept Primer: Padding and Batching\n",
        "\n",
        "### What We're Doing\n",
        "Converting variable-length encoded sequences into fixed-size tensors that neural networks can process in batches.\n",
        "\n",
        "### Why This Step is Critical\n",
        "**Neural networks require fixed-size inputs.** The challenges:\n",
        "- **Variable sequence lengths** (reviews have different numbers of words)\n",
        "- **Batch processing** (neural networks are faster with batches)\n",
        "- **Memory efficiency** (tensors must be rectangular)\n",
        "\n",
        "### What We'll Build\n",
        "- **Padding function** that extends short sequences with `<pad>` tokens\n",
        "- **Truncation logic** that cuts long sequences to max length\n",
        "- **Tensor conversion** using `torch.tensor()`\n",
        "- **DataLoader setup** for efficient batch loading\n",
        "\n",
        "### Shape Expectations\n",
        "- **Input sequences**: Variable length â†’ Fixed length (128)\n",
        "- **Tensors**: `[batch_size, sequence_length]` â†’ `[16, 128]`\n",
        "- **Labels**: `[batch_size]` â†’ `[16]`\n",
        "\n",
        "### Expected Output Example\n",
        "```python\n",
        "for X_batch, y_batch in train_dataloader:\n",
        "    print(X_batch.shape, y_batch.shape)\n",
        "# Output: torch.Size([16, 128]) torch.Size([16])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ TODO #1: Implement Padding Function\n",
        "\n",
        "**Task:** Create function that pads short sequences and truncates long ones to fixed length.\n",
        "\n",
        "**Hint:** If `len(seq) < max_len`, extend with `1` (pad token); if longer, slice `seq[:max_len]`\n",
        "\n",
        "**Expected Function Signature:**\n",
        "```python\n",
        "def pad_or_truncate(seq, max_len=128):\n",
        "    # Your implementation here\n",
        "    return padded_seq  # List of length max_len\n",
        "```\n",
        "\n",
        "**Expected Output Example:**\n",
        "```python\n",
        "pad_or_truncate([2, 3, 4], max_len=5)\n",
        "# Returns: [2, 3, 4, 1, 1]  # padded\n",
        "\n",
        "pad_or_truncate([2, 3, 4, 5, 6, 7, 8], max_len=5)\n",
        "# Returns: [2, 3, 4, 5, 6]  # truncated\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #1: Implement padding function\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ TODO #2: Encode and Pad All Texts\n",
        "\n",
        "**Task:** Apply encoding and padding to all training texts.\n",
        "\n",
        "**Hint:** Use list comprehension: `padded_text_seqs = [pad_or_truncate(encode_text(text, vocab)) for text in train_texts]`\n",
        "\n",
        "**Expected Variable:**\n",
        "- `padded_text_seqs` â†’ List of lists, each inner list has length 128\n",
        "\n",
        "**Shape Check:** All sequences should have the same length (128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #2: Encode and pad all texts\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ TODO #3: Convert to PyTorch Tensors\n",
        "\n",
        "**Task:** Convert padded sequences and labels to PyTorch tensors.\n",
        "\n",
        "**Hint:** Use `torch.tensor(padded_text_seqs, dtype=torch.long)` and `torch.tensor(train_labels, dtype=torch.long)`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `X_tensor` â†’ Tensor of shape `[num_samples, 128]`\n",
        "- `y_tensor` â†’ Tensor of shape `[num_samples]`\n",
        "\n",
        "**Data Types:** Use `torch.long` for both (integers for embeddings and class labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #3: Convert to PyTorch tensors\n",
        "import torch\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ TODO #4: Create DataLoader\n",
        "\n",
        "**Task:** Build PyTorch DataLoader for efficient batch processing.\n",
        "\n",
        "**Hint:** Use `TensorDataset(X_tensor, y_tensor)` then `DataLoader(train_dataset, batch_size=16, shuffle=True)`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `train_dataset` â†’ TensorDataset combining features and labels\n",
        "- `train_dataloader` â†’ DataLoader with batch size 16 and shuffling\n",
        "\n",
        "**Expected Output:** When you iterate through the DataLoader, you should get batches of shape `[16, 128]` and `[16]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #4: Create DataLoader\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ Reflection Prompts\n",
        "\n",
        "### ðŸ¤” Understanding Check\n",
        "1. **Why shuffle training data?** What would happen if you didn't shuffle?\n",
        "\n",
        "2. **What's the tradeoff of max_len=128 vs 256?** Consider memory usage and information loss.\n",
        "\n",
        "3. **Why use `torch.long` for both inputs and labels?** What would happen with other data types?\n",
        "\n",
        "4. **How does padding affect the model's understanding?** Will the model \"see\" the padding tokens?\n",
        "\n",
        "### ðŸŽ¯ Batching Strategy\n",
        "- Why is batch processing more efficient than processing one sample at a time?\n",
        "- How does the batch size of 16 affect training speed vs memory usage?\n",
        "- What happens if your dataset size isn't divisible by batch size?\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflections here:**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
