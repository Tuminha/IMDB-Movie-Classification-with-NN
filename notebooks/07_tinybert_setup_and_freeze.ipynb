{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Notebook 07: TinyBERT Setup and Freezing\n",
        "\n",
        "## Enter the Transformer Era\n",
        "\n",
        "This notebook introduces you to transfer learning with TinyBERT, a smaller but powerful transformer model. You'll learn how to load pre-trained models, freeze/unfreeze layers strategically, and prepare data for transformer processing with attention masks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Concept Primer: Transfer Learning with Transformers\n",
        "\n",
        "### What We're Doing\n",
        "Loading a pre-trained TinyBERT model and adapting it for our specific text classification task through strategic layer freezing and fine-tuning.\n",
        "\n",
        "### Why This Approach Works\n",
        "**Pre-trained models have learned general language representations.** Instead of training from scratch, we leverage this knowledge and adapt it to our specific task.\n",
        "\n",
        "### TinyBERT Architecture\n",
        "- **4 encoder layers** (much smaller than BERT's 12)\n",
        "- **312 hidden dimensions** (efficient but powerful)\n",
        "- **Pre-trained on large corpora** (general language understanding)\n",
        "- **Classification head** (adaptable to any number of classes)\n",
        "\n",
        "### Freezing Strategy\n",
        "1. **Freeze all parameters** initially (preserve pre-trained knowledge)\n",
        "2. **Unfreeze classifier** (adapt to our task)\n",
        "3. **Unfreeze last encoder layer** (fine-tune high-level features)\n",
        "\n",
        "### Attention Masks vs Padding\n",
        "- **Attention masks**: Tell the model which tokens to pay attention to\n",
        "- **Padding tokens**: Filler tokens to maintain fixed sequence length\n",
        "- **Key difference**: Attention masks prevent the model from \"seeing\" padding\n",
        "\n",
        "### Expected Output Example\n",
        "```python\n",
        "# Batch shapes\n",
        "input_ids.shape = torch.Size([16, 128])\n",
        "attention_mask.shape = torch.Size([16, 128])\n",
        "labels.shape = torch.Size([16])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #1: Load TinyBERT Model and Tokenizer\n",
        "\n",
        "**Task:** Load the pre-trained TinyBERT model and tokenizer from HuggingFace.\n",
        "\n",
        "**Hint:** Use `from transformers import BertTokenizer, BertForSequenceClassification` and `BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `tokenizer` ‚Üí TinyBERT tokenizer\n",
        "- `model` ‚Üí TinyBERT model with `num_labels=n_aspects`\n",
        "\n",
        "**Model Setup:** Set `num_labels=n_aspects` when loading the model for classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# TODO #1: Load TinyBERT model and tokenizer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Your code here\n",
        "tokenizer = BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
        "n_aspects = 3\n",
        "model_bert_tokenizer = BertForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels=n_aspects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #2: Freeze and Unfreeze Layers\n",
        "\n",
        "**Task:** Freeze all parameters, then unfreeze classifier and last encoder layer.\n",
        "\n",
        "**Hint:** \n",
        "- Freeze all: `for param in model.parameters(): param.requires_grad = False`\n",
        "- Unfreeze classifier: `for param in model.classifier.parameters(): param.requires_grad = True`\n",
        "- Unfreeze layer 3: `for param in model.bert.encoder.layer[3].parameters(): param.requires_grad = True`\n",
        "\n",
        "**Strategy:** Preserve most pre-trained knowledge while allowing task-specific adaptation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #2: Freeze and unfreeze layers\n",
        "# Your code here\n",
        "for param in model_bert_tokenizer.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_bert_tokenizer.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model_bert_tokenizer.bert.encoder.layer[3].parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #3: Tokenize Texts with Attention Masks\n",
        "\n",
        "**Task:** Tokenize training texts using TinyBERT tokenizer with attention masks.\n",
        "\n",
        "**Hint:** Use `tokenizer(train_texts, padding='max_length', truncation=True, max_length=128, return_tensors='pt')`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `encodings` ‚Üí Dictionary with 'input_ids' and 'attention_mask'\n",
        "- `train_dataloader` ‚Üí DataLoader with tokenized data\n",
        "\n",
        "**Key Parameters:**\n",
        "- `padding='max_length'` ‚Üí Pad to 128 tokens\n",
        "- `truncation=True` ‚Üí Cut longer sequences\n",
        "- `return_tensors='pt'` ‚Üí Return PyTorch tensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #3: Tokenize texts with attention masks\n",
        "# Your code here\n",
        "import pandas as pd\n",
        "\n",
        "train_reviews_df = pd.read_csv('../data/imdb_movie_reviews_train.csv')\n",
        "test_reviews_df = pd.read_csv('../data/imdb_movie_reviews_test.csv')\n",
        "\n",
        "# Extract text as list of strings\n",
        "train_texts = train_reviews_df['review'].tolist()  # ‚úÖ List of 369 strings\n",
        "test_texts = test_reviews_df['review'].tolist()    # ‚úÖ List of ~132 strings\n",
        "\n",
        "# Get labels too\n",
        "train_labels = train_reviews_df['aspect_encoded'].tolist()\n",
        "test_labels = test_reviews_df['aspect_encoded'].tolist()\n",
        "\n",
        "encoded_train = tokenizer(\n",
        "    train_texts,  # ‚úÖ List of strings (raw text)\n",
        "    max_length=128,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_tensors='pt'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Reflection Prompts\n",
        "\n",
        "### ü§î Understanding Check\n",
        "1. **Why freeze most layers?** What would happen if you unfroze all layers immediately?\n",
        "\n",
        "2. **What does attention_mask do differently than padding?** How does it help the model?\n",
        "\n",
        "3. **Why use TinyBERT instead of full BERT?** Consider computational efficiency vs performance.\n",
        "\n",
        "4. **How does the pre-trained vocabulary differ from your custom vocabulary?** What advantages does this provide?\n",
        "\n",
        "### üéØ Transfer Learning Strategy\n",
        "- Why is the classifier layer always unfrozen?\n",
        "- What's the benefit of unfreezing only the last encoder layer?\n",
        "- How does this approach prevent catastrophic forgetting?\n",
        "\n",
        "---\n",
        "\n",
        "## üìù My Reflections\n",
        "\n",
        "### ü§î Understanding Check Answers\n",
        "\n",
        "**1. Why freeze most layers?**\n",
        "\n",
        "We freeze most layers because they come pre-trained with valuable semantic learnings from TinyBERT. These layers already understand:\n",
        "- General language patterns and grammar\n",
        "- Word relationships and context\n",
        "- Semantic meaning and linguistic structures\n",
        "\n",
        "These pre-trained patterns are valuable and can be used across many NLP projects. By freezing them, we preserve this hard-won knowledge while adapting only what's necessary for our specific task.\n",
        "\n",
        "**What would happen if we unfroze all layers?**\n",
        "- **Catastrophic forgetting**: Model would \"forget\" pre-trained knowledge\n",
        "- **Overfitting**: With only 369 samples, we'd overfit dramatically\n",
        "- **Inefficiency**: Much slower training with marginal benefit\n",
        "- **Instability**: Training would be unstable on small datasets\n",
        "\n",
        "**2. What does attention_mask do differently than padding?**\n",
        "\n",
        "**Attention masks tell BERT which tokens are REAL vs PADDING.**\n",
        "\n",
        "**The Problem**: Without attention masks, BERT would process padding tokens like real words, wasting computation and hurting performance.\n",
        "\n",
        "**How It Works:**\n",
        "```\n",
        "Review: \"This movie was great\" ‚Üí input_ids: [101, 2023, 3185, 2001, 2307, 102, 0, 0, 0]\n",
        "                                  attention_mask: [1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
        "                                                   ‚Üë real tokens    ‚Üë padding (ignore)\n",
        "```\n",
        "\n",
        "- `1` = \"Pay attention to this token\"\n",
        "- `0` = \"Ignore this token completely\"\n",
        "\n",
        "**Inside BERT's attention mechanism:**\n",
        "- Attention scores for masked positions get -10000 added\n",
        "- After softmax, these positions get ~0 attention weight\n",
        "- BERT literally doesn't \"see\" the padding!\n",
        "\n",
        "**Difference from baseline masking:**\n",
        "- **Baseline (Notebook 05)**: Masks AFTER embedding for mean pooling\n",
        "- **TinyBERT**: Masks DURING attention computation in every layer\n",
        "- **Much more sophisticated**: Every attention head in every layer uses the mask\n",
        "\n",
        "**3. Why use TinyBERT instead of full BERT?**\n",
        "\n",
        "**TinyBERT (4 layers, 312 hidden) - Perfect for our case:**\n",
        "- ‚úÖ Small to medium datasets (369 samples)\n",
        "- ‚úÖ Simple classification tasks (3 aspects)\n",
        "- ‚úÖ Limited compute resources (CPU-friendly)\n",
        "- ‚úÖ Fast inference needed\n",
        "- ‚úÖ Good balance of performance vs efficiency\n",
        "\n",
        "**BERT-Base (12 layers, 768 hidden) - Overkill for our case:**\n",
        "- Needs 10,000s+ samples\n",
        "- Complex tasks (Question Answering, NER)\n",
        "- More compute required\n",
        "- Would be \"killing flies with a bazooka\"\n",
        "\n",
        "**BERT-Large (24 layers, 1024 hidden) - Way overkill:**\n",
        "- Needs 100,000s+ samples\n",
        "- State-of-the-art benchmarks\n",
        "- Massive compute requirements\n",
        "- Research-level projects\n",
        "\n",
        "**For our 369-sample aspect classification**: TinyBERT is the Goldilocks choice! üéØ\n",
        "\n",
        "**4. How does the pre-trained vocabulary differ from your custom vocabulary?**\n",
        "\n",
        "**My Custom Vocabulary (Baseline):**\n",
        "- 1000 most frequent words from MY training data\n",
        "- Film-specific but limited coverage\n",
        "- Many unknown words in test data (~50%+)\n",
        "- Simple tokenization (word-level)\n",
        "\n",
        "**TinyBERT's Pre-trained Vocabulary:**\n",
        "- 30,000+ subword tokens (WordPiece)\n",
        "- General language coverage\n",
        "- Can handle ANY word by breaking into subwords\n",
        "- Handles contractions, emojis, rare words better\n",
        "- Example: \"cinematography\" ‚Üí [\"cinem\", \"##ato\", \"##graphy\"]\n",
        "\n",
        "**Advantages:**\n",
        "- No unknown words (everything can be represented)\n",
        "- Better handling of rare and out-of-vocabulary words\n",
        "- Richer semantic representations\n",
        "- Pre-trained embeddings capture meaning\n",
        "\n",
        "### üéØ Transfer Learning Strategy Analysis\n",
        "\n",
        "**Why is the classifier layer always unfrozen?**\n",
        "\n",
        "The classifier layer is BRAND NEW and task-specific:\n",
        "- Pre-trained model doesn't know about our 3 aspects\n",
        "- Needs to learn from scratch: Cinematography, Characters, Story\n",
        "- Random initialization requires training\n",
        "- Maps BERT's representations to our specific classes\n",
        "\n",
        "**What's the benefit of unfreezing only the last encoder layer (Layer 3)?**\n",
        "\n",
        "**The Feature Hierarchy:**\n",
        "```\n",
        "Layer 0: Basic word patterns (\"the\", \"movie\", \"was\")\n",
        "         ‚Üì FROZEN - preserve general knowledge\n",
        "Layer 1: Word combinations, grammar (\"the movie\")\n",
        "         ‚Üì FROZEN - preserve linguistic patterns\n",
        "Layer 2: Sentence structure (\"the movie was great\")\n",
        "         ‚Üì FROZEN - preserve semantic understanding\n",
        "Layer 3: Abstract concepts (positive sentiment about films)\n",
        "         ‚Üì UNFROZEN - adapt to aspect classification\n",
        "Classifier: Cinematography/Characters/Story (task-specific)\n",
        "         ‚Üì UNFROZEN - learn from scratch\n",
        "```\n",
        "\n",
        "**Why Layer 3 is special:**\n",
        "- Learns the MOST ABSTRACT, task-relevant features\n",
        "- Closest to the classifier, so adaptation helps most\n",
        "- High-level semantic patterns that need task-specific tuning\n",
        "- Can adapt without forgetting core language knowledge\n",
        "\n",
        "**Benefits:**\n",
        "- Balances adaptation with preservation\n",
        "- Computationally efficient (fewer parameters to update)\n",
        "- Reduces overfitting risk on small datasets\n",
        "- Faster training than full fine-tuning\n",
        "\n",
        "**How does this approach prevent catastrophic forgetting?**\n",
        "\n",
        "**Catastrophic forgetting** happens when a model \"forgets\" pre-trained knowledge while learning new tasks.\n",
        "\n",
        "**Why our approach is safe:**\n",
        "\n",
        "1. **Layers 0-2 stay frozen** ‚Üí Core language knowledge preserved (0% forgetting)\n",
        "2. **Only Layer 3 + Classifier adapt** ‚Üí Limited parameter updates (~10-20% change in Layer 3)\n",
        "3. **Small learning rate (2.5e-3)** ‚Üí Gentle updates, not drastic changes\n",
        "4. **Few epochs** ‚Üí Not enough time to drastically overwrite pre-trained weights\n",
        "5. **Small dataset (369 samples)** ‚Üí Limited examples to \"overwrite\" pre-trained patterns\n",
        "\n",
        "**The Math:**\n",
        "```\n",
        "Frozen layers (0-2): 0% forgetting (no gradient updates)\n",
        "Layer 3: ~10-20% parameter change (gentle adaptation)\n",
        "Classifier: 100% new (learns from scratch)\n",
        "\n",
        "Overall: ~95% of model parameters unchanged!\n",
        "```\n",
        "\n",
        "**Analogy:**\n",
        "Think of learning a new accent:\n",
        "- **Layers 0-2** (frozen): Your core language skills (grammar, vocabulary) - never forget\n",
        "- **Layer 3** (unfrozen): Your pronunciation style - adapts to new accent\n",
        "- **Classifier** (new): Your specific phrases for this conversation\n",
        "\n",
        "You adapt your pronunciation without forgetting your native language!\n",
        "\n",
        "**Key insight:** By freezing most layers and only fine-tuning the top layer + classifier, we get the best of both worlds: adaptation to our specific task while preserving general language understanding. This is the power of transfer learning!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
