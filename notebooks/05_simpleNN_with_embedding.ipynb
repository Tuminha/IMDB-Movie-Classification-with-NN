{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Notebook 05: Simple Neural Network with Embedding\n",
        "\n",
        "## Building Your First NLP Model\n",
        "\n",
        "This notebook teaches you how to build a neural network specifically designed for text classification. You'll create an embedding layer that learns dense representations of words, then use mean pooling to aggregate sequences into fixed-size vectors for classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Concept Primer: Neural Network Architecture\n",
        "\n",
        "### What We're Building\n",
        "A neural network that transforms sequences of word IDs into aspect predictions through learned embeddings and mean pooling.\n",
        "\n",
        "### Why This Architecture Works\n",
        "**Embeddings learn semantic relationships.** Instead of treating words as discrete symbols, embeddings represent them as dense vectors where similar words have similar representations.\n",
        "\n",
        "### Architecture Flow\n",
        "1. **Input**: Word IDs `[batch, seq_len]` ‚Üí `[16, 128]`\n",
        "2. **Embedding**: Lookup table ‚Üí `[batch, seq_len, embed_dim]` ‚Üí `[16, 128, 50]`\n",
        "3. **Masking**: Ignore padding tokens during pooling\n",
        "4. **Mean Pool**: Aggregate sequence ‚Üí `[batch, embed_dim]` ‚Üí `[16, 50]`\n",
        "5. **Linear Layers**: Classification ‚Üí `[batch, n_classes]` ‚Üí `[16, n_aspects]`\n",
        "\n",
        "### Math Mapping\n",
        "- **Embedding**: `nn.Embedding(vocab_size, embed_dim)` ‚Üí lookup table\n",
        "- **Masking**: `(x != pad_id).unsqueeze(-1).float()` ‚Üí `[batch, seq, 1]`\n",
        "- **Mean Pool**: `(embedded * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)`\n",
        "- **Classification**: `Linear(embed_dim, hidden) ‚Üí ReLU ‚Üí Linear(hidden, n_classes)`\n",
        "\n",
        "### Common Pitfalls\n",
        "- **Forgetting `.clamp(min=1)`** causes division by zero for empty sequences\n",
        "- **Wrong padding_idx** in embedding layer breaks gradient flow\n",
        "- **Incorrect tensor shapes** cause runtime errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #1: Define Model Class\n",
        "\n",
        "**Task:** Create the neural network class with embedding layer and linear layers.\n",
        "\n",
        "**Hint:** Use `class SimpleNNWithEmbedding(nn.Module):` with `__init__(self, vocab_size, embed_size=50, hidden_size=100, output_size, pad_id=1)`\n",
        "\n",
        "**Expected Class Structure:**\n",
        "```python\n",
        "class SimpleNNWithEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=50, hidden_size=100, output_size, pad_id=1):\n",
        "        super().__init__()\n",
        "        # TODO: Define layers here\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward pass\n",
        "        return logits\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #1: Define model class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #2: Implement Forward Pass\n",
        "\n",
        "**Task:** Complete the forward method with embedding lookup, masking, mean pooling, and classification.\n",
        "\n",
        "**Shape Hints:**\n",
        "- `embedded = self.embedding(x)` ‚Üí `[batch, seq, embed]`\n",
        "- `mask = (x != self.pad_id).unsqueeze(-1).float()` ‚Üí `[batch, seq, 1]`\n",
        "- `masked_sum = (embedded * mask).sum(dim=1)` ‚Üí `[batch, embed]`\n",
        "- `count = mask.sum(dim=1).clamp(min=1)`\n",
        "- `pooled = masked_sum / count` ‚Üí `[batch, embed]`\n",
        "- Pass through Linear1+ReLU, then Linear2 ‚Üí `[batch, n_aspects]`\n",
        "\n",
        "**Critical:** Don't forget `.clamp(min=1)` to prevent division by zero!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #2: Implement forward pass\n",
        "# Add your forward method implementation to the class above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #3: Instantiate Model and Training Components\n",
        "\n",
        "**Task:** Create model instance, loss function, and optimizer.\n",
        "\n",
        "**Hint:** \n",
        "- `model = SimpleNNWithEmbedding(vocab_size=len(vocab), output_size=n_aspects)`\n",
        "- `criterion = nn.CrossEntropyLoss()`\n",
        "- `optimizer = torch.optim.Adam(model.parameters(), lr=0.005)`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `model` ‚Üí Instantiated neural network\n",
        "- `criterion` ‚Üí Cross-entropy loss function\n",
        "- `optimizer` ‚Üí Adam optimizer with learning rate 0.005\n",
        "\n",
        "**Test:** Try `model(X_batch)` to verify forward pass returns shape `[16, n_aspects]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #3: Instantiate model and training components\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Reflection Prompts\n",
        "\n",
        "### ü§î Understanding Check\n",
        "1. **Why mask padding tokens in mean pooling?** What would happen if you included padding in the average?\n",
        "\n",
        "2. **What does the embedding layer actually learn?** How does it differ from one-hot encoding?\n",
        "\n",
        "3. **Why use mean pooling instead of just taking the last token?** What information would you lose?\n",
        "\n",
        "4. **How does the embedding dimension (50) affect model capacity?** What's the tradeoff?\n",
        "\n",
        "### üéØ Architecture Design\n",
        "- Why is this architecture well-suited for text classification?\n",
        "- How does mean pooling handle variable-length sequences?\n",
        "- What would happen if you used max pooling instead of mean pooling?\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflections here:**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
