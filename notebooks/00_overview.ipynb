{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¬ IMDB Movie Reviews â€” Learning Journey Overview\n",
        "\n",
        "## Your NLP Learning Adventure\n",
        "\n",
        "Welcome to a **hands-on journey** from basic text processing to transformer fine-tuning. This repository teaches you NLP by building every component yourself, then comparing your baseline with TinyBERT.\n",
        "\n",
        "**Learning Philosophy:** Explain first, code later. Each notebook starts with concepts, then guides you through implementation with hintsâ€”no complete solutions provided.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  Concept Primer: The Complete NLP Pipeline\n",
        "\n",
        "### What We're Building\n",
        "A complete text classification pipeline that transforms raw movie reviews into aspect predictions, comparing two approaches:\n",
        "1. **Baseline**: Hand-crafted neural network with embeddings\n",
        "2. **TinyBERT**: Pre-trained transformer fine-tuned for the task\n",
        "\n",
        "### Why This Approach?\n",
        "Understanding each step prevents you from treating NLP as a \"black box.\" When something breaks (and it will!), you'll know exactly where to look.\n",
        "\n",
        "### The 9-Step Pipeline\n",
        "\n",
        "**Phase 1: Data Preparation (Notebooks 01-04)**\n",
        "1. **Import & Inspect** â†’ Load CSVs, understand the data structure\n",
        "2. **Tokenization** â†’ Split text into processable word units  \n",
        "3. **Vocabulary Building** â†’ Create wordâ†’integer mappings\n",
        "4. **Padding & Tensors** â†’ Convert to fixed-size tensors for neural networks\n",
        "\n",
        "**Phase 2: Baseline Model (Notebooks 05-06)**\n",
        "5. **Neural Network** â†’ Embedding layer + mean pooling + classifier\n",
        "6. **Training & Evaluation** â†’ Train loop + metrics computation\n",
        "\n",
        "**Phase 3: Transformer Comparison (Notebooks 07-09)**\n",
        "7. **TinyBERT Setup** â†’ Load pre-trained model, freeze/unfreeze layers\n",
        "8. **Fine-tuning** â†’ Adapt transformer to our specific task\n",
        "9. **Comparison** â†’ Side-by-side performance analysis\n",
        "\n",
        "### How It Maps to Math\n",
        "- **Text â†’ Tokens**: String splitting with regex\n",
        "- **Tokens â†’ Integers**: Dictionary lookup (vocab mapping)\n",
        "- **Integers â†’ Embeddings**: Lookup table â†’ dense vectors\n",
        "- **Embeddings â†’ Features**: Mean pooling â†’ fixed-size representation\n",
        "- **Features â†’ Predictions**: Linear layers â†’ class probabilities\n",
        "- **Predictions â†’ Loss**: Cross-entropy â†’ optimization signal\n",
        "\n",
        "### Where It Maps to PyTorch Objects\n",
        "- `torch.nn.Embedding` â†’ word ID lookup table\n",
        "- `torch.nn.Linear` â†’ weight matrices for classification\n",
        "- `torch.nn.CrossEntropyLoss` â†’ loss computation\n",
        "- `torch.optim.Adam` â†’ gradient descent optimizer\n",
        "- `torch.utils.data.DataLoader` â†’ batch management\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“‹ Checklist Objectives\n",
        "\n",
        "By the end of this overview, you should be able to:\n",
        "\n",
        "- [ ] **Narrate the pipeline** in 60-90 seconds without looking at notes\n",
        "- [ ] **Explain why each step exists** and what breaks without it\n",
        "- [ ] **Identify the two main approaches** (baseline vs transformer)\n",
        "- [ ] **Understand the learning philosophy** (explain-first, code-later)\n",
        "\n",
        "## âœ… Acceptance Criteria\n",
        "\n",
        "**You've mastered this notebook when you can:**\n",
        "- Recite the 9-step pipeline from memory\n",
        "- Explain the difference between baseline and TinyBERT approaches\n",
        "- Articulate why we build from scratch before using pre-trained models\n",
        "- Feel confident about the learning journey ahead\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ Pipeline Visual Diagram\n",
        "\n",
        "```\n",
        "Raw Text: \"This movie was amazing!\"\n",
        "    â†“\n",
        "[01] Load Data â†’ DataFrame with columns: review, aspect, aspect_encoded\n",
        "    â†“\n",
        "[02] Tokenize â†’ [\"this\", \"movie\", \"was\", \"amazing\"]\n",
        "    â†“\n",
        "[03] Build Vocab â†’ {\"this\": 2, \"movie\": 3, \"was\": 4, \"amazing\": 5, \"<unk>\": 0, \"<pad>\": 1}\n",
        "    â†“\n",
        "[04] Encode & Pad â†’ [2, 3, 4, 5, 1, 1, 1, ...] (length 128)\n",
        "    â†“\n",
        "[05] Neural Network â†’ Embedding(50) â†’ Mean Pool â†’ Linear(100) â†’ Linear(n_classes)\n",
        "    â†“\n",
        "[06] Train & Evaluate â†’ Loss optimization â†’ Accuracy/F1 metrics\n",
        "    â†“\n",
        "[07] TinyBERT Setup â†’ Load pre-trained â†’ Freeze most layers â†’ Unfreeze classifier\n",
        "    â†“\n",
        "[08] Fine-tune â†’ High LR (2.5e-3) â†’ Adapt to task â†’ Track loss\n",
        "    â†“\n",
        "[09] Compare â†’ Baseline vs TinyBERT â†’ Performance analysis â†’ Insights\n",
        "```\n",
        "\n",
        "**Key Insight:** Notice how both paths (baseline and TinyBERT) converge at the same evaluation metrics, but they take very different routes to get there!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ Reflection Prompts\n",
        "\n",
        "Take 5-10 minutes to reflect on these questions. Write your thoughts in the cell below:\n",
        "\n",
        "### ğŸ¤” Understanding Check\n",
        "1. **In your own words, why do we need a vocabulary?** What would happen if we tried to feed raw text directly to a neural network?\n",
        "\n",
        "2. **What's the fundamental difference between the baseline encoder and TinyBERT?** Think about what each approach \"knows\" before training starts.\n",
        "\n",
        "3. **Why do we build from scratch first, then use pre-trained models?** What would you miss if you jumped straight to TinyBERT?\n",
        "\n",
        "4. **Looking at the pipeline diagram above, which step seems most mysterious to you right now?** What would you like to understand better?\n",
        "\n",
        "### ğŸ¯ Personal Learning Goals\n",
        "- What aspect of NLP are you most excited to learn about?\n",
        "- What's your biggest concern about this learning journey?\n",
        "- How do you plan to use these notebooks to build understanding (not just copy code)?\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflections here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸš€ Ready to Begin?\n",
        "\n",
        "You're now equipped with the mental model for the entire NLP pipeline. When you're ready to start building, move on to **Notebook 01: Import and Inspect** where you'll load your first dataset and begin the hands-on journey.\n",
        "\n",
        "**Remember:** Each notebook builds on the previous one. Don't skip aheadâ€”the learning is in the journey, not just the destination.\n",
        "\n",
        "**Next up:** Data familiarization and the first glimpse of your movie reviews dataset! ğŸ¬\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
