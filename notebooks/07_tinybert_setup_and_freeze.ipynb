{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Notebook 07: TinyBERT Setup and Freezing\n",
        "\n",
        "## Enter the Transformer Era\n",
        "\n",
        "This notebook introduces you to transfer learning with TinyBERT, a smaller but powerful transformer model. You'll learn how to load pre-trained models, freeze/unfreeze layers strategically, and prepare data for transformer processing with attention masks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Concept Primer: Transfer Learning with Transformers\n",
        "\n",
        "### What We're Doing\n",
        "Loading a pre-trained TinyBERT model and adapting it for our specific text classification task through strategic layer freezing and fine-tuning.\n",
        "\n",
        "### Why This Approach Works\n",
        "**Pre-trained models have learned general language representations.** Instead of training from scratch, we leverage this knowledge and adapt it to our specific task.\n",
        "\n",
        "### TinyBERT Architecture\n",
        "- **4 encoder layers** (much smaller than BERT's 12)\n",
        "- **312 hidden dimensions** (efficient but powerful)\n",
        "- **Pre-trained on large corpora** (general language understanding)\n",
        "- **Classification head** (adaptable to any number of classes)\n",
        "\n",
        "### Freezing Strategy\n",
        "1. **Freeze all parameters** initially (preserve pre-trained knowledge)\n",
        "2. **Unfreeze classifier** (adapt to our task)\n",
        "3. **Unfreeze last encoder layer** (fine-tune high-level features)\n",
        "\n",
        "### Attention Masks vs Padding\n",
        "- **Attention masks**: Tell the model which tokens to pay attention to\n",
        "- **Padding tokens**: Filler tokens to maintain fixed sequence length\n",
        "- **Key difference**: Attention masks prevent the model from \"seeing\" padding\n",
        "\n",
        "### Expected Output Example\n",
        "```python\n",
        "# Batch shapes\n",
        "input_ids.shape = torch.Size([16, 128])\n",
        "attention_mask.shape = torch.Size([16, 128])\n",
        "labels.shape = torch.Size([16])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #1: Load TinyBERT Model and Tokenizer\n",
        "\n",
        "**Task:** Load the pre-trained TinyBERT model and tokenizer from HuggingFace.\n",
        "\n",
        "**Hint:** Use `from transformers import BertTokenizer, BertForSequenceClassification` and `BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `tokenizer` ‚Üí TinyBERT tokenizer\n",
        "- `model` ‚Üí TinyBERT model with `num_labels=n_aspects`\n",
        "\n",
        "**Model Setup:** Set `num_labels=n_aspects` when loading the model for classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #1: Load TinyBERT model and tokenizer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #2: Freeze and Unfreeze Layers\n",
        "\n",
        "**Task:** Freeze all parameters, then unfreeze classifier and last encoder layer.\n",
        "\n",
        "**Hint:** \n",
        "- Freeze all: `for param in model.parameters(): param.requires_grad = False`\n",
        "- Unfreeze classifier: `for param in model.classifier.parameters(): param.requires_grad = True`\n",
        "- Unfreeze layer 3: `for param in model.bert.encoder.layer[3].parameters(): param.requires_grad = True`\n",
        "\n",
        "**Strategy:** Preserve most pre-trained knowledge while allowing task-specific adaptation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #2: Freeze and unfreeze layers\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #3: Tokenize Texts with Attention Masks\n",
        "\n",
        "**Task:** Tokenize training texts using TinyBERT tokenizer with attention masks.\n",
        "\n",
        "**Hint:** Use `tokenizer(train_texts, padding='max_length', truncation=True, max_length=128, return_tensors='pt')`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `encodings` ‚Üí Dictionary with 'input_ids' and 'attention_mask'\n",
        "- `train_dataloader` ‚Üí DataLoader with tokenized data\n",
        "\n",
        "**Key Parameters:**\n",
        "- `padding='max_length'` ‚Üí Pad to 128 tokens\n",
        "- `truncation=True` ‚Üí Cut longer sequences\n",
        "- `return_tensors='pt'` ‚Üí Return PyTorch tensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO #3: Tokenize texts with attention masks\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Reflection Prompts\n",
        "\n",
        "### ü§î Understanding Check\n",
        "1. **Why freeze most layers?** What would happen if you unfroze all layers immediately?\n",
        "\n",
        "2. **What does attention_mask do differently than padding?** How does it help the model?\n",
        "\n",
        "3. **Why use TinyBERT instead of full BERT?** Consider computational efficiency vs performance.\n",
        "\n",
        "4. **How does the pre-trained vocabulary differ from your custom vocabulary?** What advantages does this provide?\n",
        "\n",
        "### üéØ Transfer Learning Strategy\n",
        "- Why is the classifier layer always unfrozen?\n",
        "- What's the benefit of unfreezing only the last encoder layer?\n",
        "- How does this approach prevent catastrophic forgetting?\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflections here:**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
