{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî§ Notebook 02: Preprocessing & Tokenization\n",
        "\n",
        "## From Raw Text to Processable Tokens\n",
        "\n",
        "This notebook teaches you how to transform raw movie review text into a format that neural networks can understand. You'll build a tokenization function that splits text into individual words while handling common preprocessing challenges.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Concept Primer: Why Tokenization Matters\n",
        "\n",
        "### What We're Doing\n",
        "Converting raw text strings into lists of individual words (tokens) that can be processed by neural networks.\n",
        "\n",
        "### Why This Step is Critical\n",
        "**Neural networks can't process raw text directly.** They need:\n",
        "- **Fixed-size inputs** (tokens, not variable-length strings)\n",
        "- **Numerical representations** (integers, not text)\n",
        "- **Consistent format** (lowercase, no punctuation)\n",
        "\n",
        "### What We'll Build\n",
        "- **Tokenization function** using regex to extract words\n",
        "- **Text normalization** (lowercase conversion)\n",
        "- **Corpus creation** (list of tokenized reviews)\n",
        "\n",
        "### Common Pitfalls\n",
        "- **Forgetting to lowercase** creates duplicate vocabulary entries (\"Movie\" ‚â† \"movie\")\n",
        "- **Inconsistent tokenization** leads to vocabulary bloat\n",
        "- **Special character handling** can break downstream processing\n",
        "\n",
        "### How It Maps to Our Pipeline\n",
        "Raw text ‚Üí `tokenize_review()` ‚Üí List of tokens ‚Üí Vocabulary building (next notebook)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #1: Implement Tokenization Function\n",
        "\n",
        "**Task:** Create a function that splits text into individual words using regex.\n",
        "\n",
        "**Hint:** Use `re.findall(r'\\b\\w+\\b', text.lower())` to extract word tokens and convert to lowercase.\n",
        "\n",
        "**Expected Function Signature:**\n",
        "```python\n",
        "def tokenize_review(text):\n",
        "    # Your implementation here\n",
        "    return tokens  # List of strings\n",
        "```\n",
        "\n",
        "**Expected Output Example:**\n",
        "```python\n",
        "tokenize_review(\"This movie was amazing!\")\n",
        "# Returns: ['this', 'movie', 'was', 'amazing']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'world', 'this', 'is', 'a', 'test', 'ms', 'tokenization', 'is', 'in', 'good', 'mood', 'let', 's', 'see', 'what', 'happens']\n"
          ]
        }
      ],
      "source": [
        "# TODO #1: Implement tokenization function\n",
        "import re\n",
        "\n",
        "# Your code here\n",
        "def tokenize(text):\n",
        "    # Use regex to find words, ignoring punctuation\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "# Test the function\n",
        "sample_test = \"Hello, world! This is a test. Ms. Tokenization is in üòÅ good mood. Let's see what happens\"\n",
        "print(tokenize(sample_test))  # Expected output: ['hello', 'world', 'this', 'is', 'a', 'test']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #2: Create Tokenized Corpus\n",
        "\n",
        "**Task:** Apply your tokenization function to all training texts to create a corpus of tokenized reviews.\n",
        "\n",
        "**Hint:** Use list comprehension: `tokenized_corpus = [tokenize_review(text) for text in train_texts]`\n",
        "\n",
        "**Expected Variable:**\n",
        "- `tokenized_corpus` ‚Üí List of lists, where each inner list contains tokens from one review\n",
        "\n",
        "**Expected Output Example:**\n",
        "```python\n",
        "tokenized_corpus[:3]\n",
        "# Returns: [['this', 'movie', 'was', 'amazing'], ['terrible', 'plot', 'bad', 'acting'], ['love', 'this', 'film']]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['ibiza', 'filming', 'location', 'looks', 'very', 'enchanting']]\n"
          ]
        }
      ],
      "source": [
        "# TODO #2: Create tokenized corpus\n",
        "# Your code here\n",
        "import pandas as pd\n",
        "\n",
        "train_reviews_df = pd.read_csv('../data/imdb_movie_reviews_train.csv')\n",
        "test_reviews_df = pd.read_csv('../data/imdb_movie_reviews_test.csv')\n",
        "\n",
        "tokenized_corpus_train = train_reviews_df['review'].apply(tokenize).tolist() \n",
        "tokenized_corpus_test = test_reviews_df['review'].apply(tokenize).tolist()\n",
        "\n",
        "print(tokenized_corpus_train[:1])  # Print first tokenized review from training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù My Reflections\n",
        "\n",
        "### ü§î Understanding Check Answers\n",
        "\n",
        "1. **Why remove punctuation?** \n",
        "   - Punctuation creates noise in vocabulary and doesn't carry semantic meaning for classification\n",
        "   - If kept as separate tokens, it would bloat the vocabulary with meaningless entries like \"!\", \"?\", \",\"\n",
        "   - However, we lose emphasis and emotional cues (e.g., \"amazing!\" vs \"amazing\")\n",
        "\n",
        "2. **What information do we lose by lowercasing?**\n",
        "   - Proper nouns become generic (e.g., \"Movie\" ‚Üí \"movie\" loses the specific film reference)\n",
        "   - Acronyms lose their distinctiveness (e.g., \"IMDB\" ‚Üí \"imdb\")\n",
        "   - Some context about emphasis and importance\n",
        "\n",
        "3. **How does the regex pattern `r'\\b\\w+\\b'` work?**\n",
        "   - `\\b` = word boundary (invisible position between word and non-word characters)\n",
        "   - `\\w+` = one or more word characters (letters, digits, underscore)\n",
        "   - `\\b` = another word boundary\n",
        "   - This ensures clean tokenization at word boundaries, but still splits contractions like \"let's\" ‚Üí [\"let\", \"s\"]\n",
        "\n",
        "4. **Why is consistent tokenization important?**\n",
        "   - Inconsistent tokenization creates duplicate vocabulary entries\n",
        "   - Same word tokenized differently leads to vocabulary bloat and confusion\n",
        "   - Neural networks need consistent input representations\n",
        "\n",
        "### üéØ Tokenization Quality Assessment\n",
        "\n",
        "**What I discovered:**\n",
        "- **Contractions are problematic**: \"let's\" ‚Üí [\"let\", \"s\"] loses semantic meaning\n",
        "- **Emojis are completely removed**: üòÅ disappears, losing emotional context\n",
        "- **Standalone letters are meaningless**: \"s\" from \"let's\" provides no information\n",
        "- **Punctuation removal loses emphasis**: \"amazing!\" vs \"amazing\" are different\n",
        "\n",
        "**Edge cases my tokenizer doesn't handle well:**\n",
        "- Contractions (let's, don't, won't)\n",
        "- Possessives (John's ‚Üí [\"john\", \"s\"])\n",
        "- Emojis and special characters\n",
        "- Punctuation that carries meaning\n",
        "\n",
        "**How TinyBERT will differ:**\n",
        "- Subword tokenization keeps meaningful parts (\"let's\" ‚Üí [\"let\", \"'s\"])\n",
        "- Preserves punctuation and special characters\n",
        "- Handles contractions intelligently\n",
        "- Converts emojis to text or preserves them\n",
        "- Much more sophisticated vocabulary management\n",
        "\n",
        "**Key insight:** This simple tokenization approach will show us exactly what information is lost, making the TinyBERT comparison much more meaningful. The baseline model will struggle with contractions and lose emotional context, while TinyBERT should handle these cases much better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Reflection Prompts\n",
        "\n",
        "### ü§î Understanding Check\n",
        "1. **Why remove punctuation?** What would happen if you kept punctuation marks as separate tokens?\n",
        "\n",
        "2. **What information do we lose by lowercasing?** Think about proper nouns and acronyms.\n",
        "\n",
        "3. **How does the regex pattern `r'\\b\\w+\\b'` work?** What does `\\b` mean in regex?\n",
        "\n",
        "4. **Why is consistent tokenization important?** What happens if the same word gets tokenized differently in different contexts?\n",
        "\n",
        "### üéØ Tokenization Quality\n",
        "- Do your tokenized examples look reasonable?\n",
        "- Are there any edge cases your tokenizer might not handle well?\n",
        "- How might this tokenization approach differ from what TinyBERT will do?\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflections here:**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
