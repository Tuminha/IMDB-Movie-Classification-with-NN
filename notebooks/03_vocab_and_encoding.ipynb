{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Notebook 03: Vocabulary Building & Encoding\n",
        "\n",
        "## From Words to Numbers\n",
        "\n",
        "This notebook teaches you how to build a vocabulary dictionary that maps words to integers, then use that vocabulary to encode your tokenized text into numerical representations that neural networks can process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Concept Primer: Vocabulary and Encoding\n",
        "\n",
        "### What We're Doing\n",
        "Building a vocabulary dictionary that maps words to integers, then encoding tokenized text into numerical sequences.\n",
        "\n",
        "### Why This Step is Critical\n",
        "**Neural networks require numerical inputs.** We need to:\n",
        "- **Map words to integers** (vocabulary dictionary)\n",
        "- **Handle unknown words** (special `<unk>` token)\n",
        "- **Manage sequence length** (special `<pad>` token)\n",
        "- **Limit vocabulary size** (most common 1000 words + specials)\n",
        "\n",
        "### What We'll Build\n",
        "- **Word frequency counting** using `collections.Counter`\n",
        "- **Vocabulary dictionary** with special tokens at positions 0 and 1\n",
        "- **Encoding function** that converts tokens to integers\n",
        "- **Out-of-vocabulary handling** (unknown words ‚Üí `<unk>`)\n",
        "\n",
        "### Special Tokens Strategy\n",
        "- `<unk>` = 0 (unknown/rare words)\n",
        "- `<pad>` = 1 (padding for fixed-length sequences)\n",
        "- Regular words = 2, 3, 4, ... (most frequent first)\n",
        "\n",
        "### Expected Output Example\n",
        "```python\n",
        "encode_text(\"This movie was great\", vocab)\n",
        "# Returns: [45, 12, 8, 203] (actual numbers depend on your vocab)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #1: Build Word Frequencies\n",
        "\n",
        "**Task:** Count word frequencies across all tokenized reviews to identify the most common words.\n",
        "\n",
        "**Hint:** Flatten `tokenized_corpus` into a single list, then use `Counter(combined_corpus)`\n",
        "\n",
        "**Expected Variables:**\n",
        "- `combined_corpus` ‚Üí Flat list of all tokens from all reviews\n",
        "- `word_freqs` ‚Üí Counter object with word frequencies\n",
        "\n",
        "**Expected Output:** You should see the most frequent words when you print `word_freqs.most_common(10)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['ibiza', 'filming', 'location', 'looks', 'very', 'enchanting']]\n",
            "['ibiza', 'filming', 'location', 'looks', 'very', 'enchanting', 'randolph', 'scott', 'always', 'played', 'men', 'you', 'could', 'look', 'up', 'to', 'for', 'their', 'sense', 'of']\n",
            "[('the', 732), ('a', 307), ('and', 306), ('of', 296), ('is', 218), ('to', 213), ('in', 177), ('it', 134), ('s', 109), ('that', 105)]\n"
          ]
        }
      ],
      "source": [
        "# TODO #1: Build word frequencies\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Your code here\n",
        "import pandas as pd\n",
        "\n",
        "def tokenize(text):\n",
        "    # Use regex to find words, ignoring punctuation\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "\n",
        "train_reviews_df = pd.read_csv('../data/imdb_movie_reviews_train.csv')\n",
        "test_reviews_df = pd.read_csv('../data/imdb_movie_reviews_test.csv')\n",
        "\n",
        "tokenized_corpus_train = train_reviews_df['review'].apply(tokenize).tolist() \n",
        "tokenized_corpus_test = test_reviews_df['review'].apply(tokenize).tolist()\n",
        "\n",
        "print(tokenized_corpus_train[:1])  # Print first tokenized review from training set\n",
        "\n",
        "combined_corpus = []\n",
        "\n",
        "for word in tokenized_corpus_train:\n",
        "    combined_corpus.extend(word)\n",
        "\n",
        "print(combined_corpus[:20])  # Print first 20 words from combined corpus\n",
        "\n",
        "word_freqs = Counter(combined_corpus)\n",
        "print(word_freqs.most_common(10))  # Print 10 most common words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #2: Create Vocabulary Dictionary\n",
        "\n",
        "**Task:** Build vocabulary dictionary with most common 1000 words plus special tokens.\n",
        "\n",
        "**Hint:** Use `word_freqs.most_common(1000)` to get top words, then create vocab with `{word: idx+2 for idx, (word, _) in enumerate(most_common_words)}`, then add special tokens.\n",
        "\n",
        "**Expected Variables:**\n",
        "- `MAX_VOCAB_SIZE = 1000`\n",
        "- `most_common_words` ‚Üí List of (word, count) tuples\n",
        "- `vocab` ‚Üí Dictionary mapping words to integers\n",
        "\n",
        "**Expected Output:** `len(vocab) == 1002` (1000 words + 2 special tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('<PAD>', 0), ('<UNK>', 1), ('the', 2), ('a', 3), ('and', 4), ('of', 5), ('is', 6), ('to', 7), ('in', 8), ('it', 9)]\n",
            "Vocabulary size: 1002\n"
          ]
        }
      ],
      "source": [
        "# TODO #2: Create vocabulary dictionary\n",
        "# Your code here\n",
        "max_vocab_size = 1002\n",
        "most_common_words = word_freqs.most_common(max_vocab_size - 2)  # Reserve 2 for <PAD> and <UNK>\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1, **{word: idx + 2 for idx, (word, _) in enumerate(most_common_words)}}\n",
        "print(list(vocab.items())[:10])  # Print first 10 items in vocabulary dictionary\n",
        "vocab_size = len(vocab)\n",
        "print(f'Vocabulary size: {vocab_size}')  # Print vocabulary size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß TODO #3: Implement Encoding Function\n",
        "\n",
        "**Task:** Create function that converts text to list of integers using vocabulary.\n",
        "\n",
        "**Hint:** Tokenize first with your existing function, then use `[vocab.get(token, 0) for token in tokens]` to handle unknown words.\n",
        "\n",
        "**Expected Function Signature:**\n",
        "```python\n",
        "def encode_text(text, vocab):\n",
        "    # Your implementation here\n",
        "    return encoded_ids  # List of integers\n",
        "```\n",
        "\n",
        "**Expected Output Example:**\n",
        "```python\n",
        "encode_text(\"This movie was great\", vocab)\n",
        "# Returns: [45, 12, 8, 203] (numbers will vary based on your vocab)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of <UNK> tokens in training set: 2027\n",
            "Percentage of <UNK> tokens in training set: 18.86%\n",
            "[[1, 252, 591, 114, 26, 1]]\n",
            "[[2, 91, 6, 1, 9, 34, 440, 125, 170, 1]]\n"
          ]
        }
      ],
      "source": [
        "# TODO #3: Implement encoding function\n",
        "# Your code here\n",
        "def encode_text(text, vocab):\n",
        "    tokens = tokenize(text)\n",
        "    encoded = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "    return encoded\n",
        "\n",
        "encoded_reviews_train = train_reviews_df['review'].apply(lambda x: encode_text(x, vocab)).tolist()\n",
        "encoded_reviews_test = test_reviews_df['review'].apply(lambda x: encode_text(x, vocab)).tolist()\n",
        "\n",
        "num_unk = sum(token == vocab['<UNK>'] for review in encoded_reviews_train for token in review)\n",
        "print(f'Number of <UNK> tokens in training set: {num_unk}')  # Print number of <UNK> tokens\n",
        "print(f\"Percentage of <UNK> tokens in training set: {num_unk / sum(len(review) for review in encoded_reviews_train) * 100:.2f}%\")\n",
        "\n",
        "print(encoded_reviews_train[:1])  # Print first encoded review from training set\n",
        "print(encoded_reviews_test[:1])  # Print first encoded review from test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Reflection Prompts\n",
        "\n",
        "### ü§î Understanding Check\n",
        "1. **Why start vocab IDs at 2?** What would happen if you started at 0?\n",
        "\n",
        "2. **What happens if test data has many unknown words?** How does this affect model performance?\n",
        "\n",
        "3. **Why limit vocabulary to 1000 words?** What's the tradeoff between vocabulary size and model performance?\n",
        "\n",
        "4. **How does the `<unk>` token help with generalization?** What would happen without it?\n",
        "\n",
        "### üéØ Vocabulary Quality\n",
        "- Are the most frequent words what you'd expect for movie reviews?\n",
        "- How many words in your vocabulary do you recognize as movie-related?\n",
        "- What percentage of tokens will become `<unk>` in unseen text?\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflections here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù My Reflections\n",
        "\n",
        "### ü§î Understanding Check Answers\n",
        "\n",
        "1. **Why start vocab IDs at 2?** \n",
        "   - If we started at 0, we would overwrite the special tokens `<PAD>` and `<UNK>`\n",
        "   - Special tokens need reserved positions (0 and 1) to handle edge cases consistently\n",
        "   - Starting at 2 ensures regular words don't conflict with special token functionality\n",
        "\n",
        "2. **What happens if test data has many unknown words?**\n",
        "   - If test data has too many unknown words, the trained model will struggle to make predictions\n",
        "   - The core words needed for classification might not be in the vocabulary\n",
        "   - This highlights the importance of having a representative vocabulary from training data\n",
        "\n",
        "3. **Why limit vocabulary to 1000 words?**\n",
        "   - We get a representative vocabulary of the most important words\n",
        "   - We remove noise from training by filtering out rare words\n",
        "   - Balances vocabulary coverage with computational efficiency\n",
        "   - Prevents overfitting to very rare words that might not generalize\n",
        "\n",
        "4. **How does the `<unk>` token help with generalization?**\n",
        "   - Actually, `<UNK>` doesn't directly help with generalization - it's more about handling edge cases\n",
        "   - It prevents crashes when encountering unknown words during inference\n",
        "   - It provides a consistent fallback for words not seen during training\n",
        "   - The real generalization comes from having a diverse, representative vocabulary\n",
        "\n",
        "### üéØ Vocabulary Quality Assessment\n",
        "\n",
        "**Most frequent words analysis:**\n",
        "- The most frequent words are indeed what I expected: mainly stopwords (\"the\", \"a\", \"and\", \"of\", \"is\")\n",
        "- These are common English words that appear frequently across all types of text\n",
        "- Stopwords provide grammatical structure but less semantic meaning for classification\n",
        "\n",
        "**Movie-related vocabulary:**\n",
        "- I don't have a specific count of movie-related words, but I'd be interested to learn how to analyze this\n",
        "- Could potentially filter vocabulary for domain-specific terms related to cinematography, characters, story\n",
        "\n",
        "**Unknown word prediction:**\n",
        "- In unseen text, I believe we'll have at least 50% unknown words\n",
        "- This is because our vocabulary only covers the top 1000 most frequent words from training\n",
        "- Many domain-specific terms, proper nouns, and less common words will become `<UNK>`\n",
        "- This limitation will be interesting to compare with TinyBERT's subword approach\n",
        "\n",
        "**Key insight:** The vocabulary limitation highlights why transformer models with subword tokenization often perform better - they can handle unknown words by breaking them into meaningful subword pieces rather than losing all information with `<UNK>` tokens.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
